{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Python Modules Installation"
      ],
      "metadata": {
        "id": "5wgUwxj7XUdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb sentence-transformers PyPDF2 openai tqdm pandas ragas datasets"
      ],
      "metadata": {
        "id": "uF4drbGYNff_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Setting up the Open AI API Key"
      ],
      "metadata": {
        "id": "Kfgy6zVjXxvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "# Load API key from secrets\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Quick test\n",
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    max_tokens=5\n",
        ")\n",
        "print(\"âœ… OpenAI API working!\")"
      ],
      "metadata": {
        "id": "tNKPewH-NmA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading the 5 Internal Docs files"
      ],
      "metadata": {
        "id": "YZ99XsoVYBk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"ðŸ“š Upload your mainframe migration documentation (.md files):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process uploaded markdown files\n",
        "md_files = []\n",
        "for filename, content in uploaded.items():\n",
        "    if filename.endswith('.md'):\n",
        "        # Decode binary content to text for markdown files\n",
        "        text_content = content.decode('utf-8')\n",
        "        with open(f\"/content/{filename}\", 'w', encoding='utf-8') as f:\n",
        "            f.write(text_content)\n",
        "        md_files.append(f\"/content/{filename}\")\n",
        "        print(f\"âœ… {filename}\")\n",
        "\n",
        "print(f\"ðŸ“Š Ready to process {len(md_files)} markdown files\")\n",
        "\n",
        "# Optional: Display the files that will be processed\n",
        "if md_files:\n",
        "    print(\"\\nðŸ“‹ Uploaded files:\")\n",
        "    for file_path in md_files:\n",
        "        filename = os.path.basename(file_path)\n",
        "        file_size = os.path.getsize(file_path)\n",
        "        print(f\"  â€¢ {filename} ({file_size} bytes)\")\n",
        "else:\n",
        "    print(\"âš ï¸  No markdown files were uploaded. Please upload .md files.\")"
      ],
      "metadata": {
        "id": "I7WvPorONoBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sets up a ChromaDB vector database with OpenAI embeddings"
      ],
      "metadata": {
        "id": "lxgB1AhPYRjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import openai\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "print(\"ðŸš€ Setting up ChromaDB with OpenAI Embeddings\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# CHROMADB SETUP\n",
        "# ==========================================\n",
        "\n",
        "# Simple in-memory vector database\n",
        "client = chromadb.Client()\n",
        "\n",
        "# Collection name\n",
        "collection_name = \"aws_docs\"\n",
        "\n",
        "# Check if collection exists and delete it\n",
        "try:\n",
        "    # Get list of existing collections\n",
        "    existing_collections = client.list_collections()\n",
        "    collection_names = [col.name for col in existing_collections]\n",
        "\n",
        "    if collection_name in collection_names:\n",
        "        print(f\"ðŸ—‘ï¸ Deleting existing collection: {collection_name}\")\n",
        "        client.delete_collection(name=collection_name)\n",
        "        print(f\"âœ… Collection '{collection_name}' deleted successfully\")\n",
        "    else:\n",
        "        print(f\"â„¹ï¸ No existing collection named '{collection_name}' found\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Error checking/deleting collection: {e}\")\n",
        "\n",
        "# Create new collection\n",
        "try:\n",
        "    collection = client.create_collection(name=collection_name)\n",
        "    print(f\"âœ… Created new collection: {collection_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error creating collection: {e}\")\n",
        "    # Fallback: try to get existing collection\n",
        "    try:\n",
        "        collection = client.get_collection(name=collection_name)\n",
        "        print(f\"ðŸ“ Using existing collection: {collection_name}\")\n",
        "    except Exception as fallback_error:\n",
        "        print(f\"âŒ Could not create or access collection: {fallback_error}\")\n",
        "        raise\n",
        "\n",
        "# ==========================================\n",
        "# OPENAI EMBEDDING MODEL SETUP\n",
        "# ==========================================\n",
        "\n",
        "class OpenAIEmbeddingModel:\n",
        "    \"\"\"OpenAI embedding model wrapper compatible with existing pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, model: str = \"text-embedding-3-small\"):\n",
        "        self.model = model\n",
        "        self.embedding_dimension = None\n",
        "\n",
        "        # Test OpenAI connection and get embedding dimension\n",
        "        try:\n",
        "            print(f\"ðŸ”„ Testing OpenAI {model} connection...\")\n",
        "            test_response = openai.embeddings.create(\n",
        "                model=self.model,\n",
        "                input=[\"test connection\"]\n",
        "            )\n",
        "            self.embedding_dimension = len(test_response.data[0].embedding)\n",
        "            print(f\"âœ… OpenAI {model} connected successfully!\")\n",
        "            print(f\"ðŸ“ Embedding dimension: {self.embedding_dimension}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ OpenAI connection failed: {e}\")\n",
        "            print(\"ðŸ”‘ Make sure OPENAI_API_KEY is set in Colab secrets\")\n",
        "            raise\n",
        "\n",
        "    def encode(self, texts, show_progress_bar=False, convert_to_numpy=True):\n",
        "        \"\"\"Generate embeddings for texts (compatible with sentence-transformers interface)\"\"\"\n",
        "\n",
        "        # Handle single string input\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        try:\n",
        "            response = openai.embeddings.create(\n",
        "                model=self.model,\n",
        "                input=texts\n",
        "            )\n",
        "\n",
        "            # Extract embeddings\n",
        "            embeddings = []\n",
        "            for item in response.data:\n",
        "                embeddings.append(item.embedding)\n",
        "\n",
        "            if convert_to_numpy:\n",
        "                return np.array(embeddings)\n",
        "            else:\n",
        "                return embeddings\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ OpenAI embedding error: {e}\")\n",
        "            return np.array([]) if convert_to_numpy else []\n",
        "\n",
        "    def get_sentence_embedding_dimension(self):\n",
        "        \"\"\"Get embedding dimension\"\"\"\n",
        "        return self.embedding_dimension\n",
        "\n",
        "# Load OpenAI embedding model\n",
        "try:\n",
        "    embedding_model = OpenAIEmbeddingModel(\"text-embedding-3-large\")\n",
        "    print(\"âœ… OpenAI embedding model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading OpenAI embedding model: {e}\")\n",
        "    print(\"ðŸ”„ Falling back to sentence-transformers model...\")\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"âœ… Fallback: Sentence-transformers model loaded\")\n",
        "    except Exception as fallback_error:\n",
        "        print(f\"âŒ Both OpenAI and sentence-transformers failed: {fallback_error}\")\n",
        "        raise\n",
        "\n",
        "# ==========================================\n",
        "# EMBEDDING QUALITY TEST\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\nðŸ§ª Testing embedding quality...\")\n",
        "\n",
        "# Test the problematic query\n",
        "\n",
        "query = \"What is AWS Transform?\"\n",
        "test_content = \"AWS Transform automates COBOL to Java conversion with AI-powered code analysis and modernization.\"\n",
        "try:\n",
        "    # Generate embeddings\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    content_embedding = embedding_model.encode([test_content])\n",
        "\n",
        "    # Calculate similarity\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    similarity = cosine_similarity(query_embedding, content_embedding)[0][0]\n",
        "\n",
        "    print(f\"ðŸŽ¯ Embedding quality test:\")\n",
        "    print(f\"   Query: '{query}'\")\n",
        "    print(f\"   Similarity score: {similarity:.3f}\")\n",
        "\n",
        "    if similarity > 0.8:\n",
        "        print(\"ðŸŽ‰ EXCELLENT! High-quality embeddings\")\n",
        "    elif similarity > 0.7:\n",
        "        print(\"âœ… GOOD! Strong semantic matching\")\n",
        "    elif similarity > 0.5:\n",
        "        print(\"ðŸŸ¡ MODERATE: Acceptable but could be better\")\n",
        "    else:\n",
        "        print(\"âŒ LOW: Embeddings may not work well for this use case\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Embedding test failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸŽ‰ ChromaDB + OpenAI Embedding Setup Complete!\")\n",
        "print(f\"ðŸ“Š Collection: {collection_name}\")\n",
        "print(f\"ðŸ¤– Model: {embedding_model.model if hasattr(embedding_model, 'model') else 'sentence-transformers'}\")\n",
        "print(f\"ðŸ“ Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
        "print(\"ðŸ“ Ready for improved document storage and retrieval\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Store key variables for next phases\n",
        "setup_complete = True\n",
        "print(f\"\\nðŸ’¾ Variables ready for next phases:\")\n",
        "print(f\"   âœ… client (ChromaDB client)\")\n",
        "print(f\"   âœ… collection ('{collection_name}' collection)\")\n",
        "print(f\"   âœ… embedding_model (OpenAI or sentence-transformers)\")\n",
        "print(f\"   âœ… setup_complete = {setup_complete}\")"
      ],
      "metadata": {
        "id": "x9wXr1Dz_ymO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunks markdown files into text segments for vector database storage."
      ],
      "metadata": {
        "id": "zFmvv7vcZG3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Document Processing Pipeline\n",
        "# ==========================================\n",
        "\n",
        "import re\n",
        "import tiktoken\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import hashlib\n",
        "\n",
        "print(\"ðŸš€ Phase 1B: Document Processing Pipeline (Markdown)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ==========================================\n",
        "# DOCUMENT CHUNK DATA STRUCTURE\n",
        "# ==========================================\n",
        "\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    \"\"\"Structured document chunk with metadata\"\"\"\n",
        "    id: str\n",
        "    content: str\n",
        "    source_file: str\n",
        "    page_number: int  # For markdown, this will be section number\n",
        "    chunk_index: int\n",
        "    token_count: int\n",
        "    char_count: int\n",
        "    section_title: Optional[str] = None\n",
        "    chunk_type: str = \"content\"  # content, header, code, etc.\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        \"\"\"Convert to dictionary for ChromaDB storage\"\"\"\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'content': self.content,\n",
        "            'source_file': self.source_file,\n",
        "            'page_number': self.page_number,\n",
        "            'chunk_index': self.chunk_index,\n",
        "            'token_count': self.token_count,\n",
        "            'char_count': self.char_count,\n",
        "            'section_title': self.section_title or \"\",\n",
        "            'chunk_type': self.chunk_type\n",
        "        }\n",
        "\n",
        "# ==========================================\n",
        "# MARKDOWN TEXT EXTRACTION\n",
        "# ==========================================\n",
        "\n",
        "class MarkdownProcessor:\n",
        "    \"\"\"Advanced Markdown text extraction with metadata preservation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT tokenizer\n",
        "\n",
        "    def extract_text_from_markdown(self, md_path: str) -> List[Dict]:\n",
        "        \"\"\"Extract text from Markdown with section-level metadata\"\"\"\n",
        "\n",
        "        print(f\"ðŸ“„ Processing: {md_path}\")\n",
        "\n",
        "        try:\n",
        "            with open(md_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            # Split markdown into sections based on headers\n",
        "            sections_data = self._parse_markdown_sections(content)\n",
        "\n",
        "            print(f\"âœ… Extracted {len(sections_data)} sections from {md_path}\")\n",
        "            return sections_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error processing {md_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _parse_markdown_sections(self, content: str) -> List[Dict]:\n",
        "        \"\"\"Parse markdown content into sections based on headers\"\"\"\n",
        "\n",
        "        sections_data = []\n",
        "        lines = content.split('\\n')\n",
        "\n",
        "        current_section = {\n",
        "            'section_number': 1,\n",
        "            'header': None,\n",
        "            'content': '',\n",
        "            'level': 0\n",
        "        }\n",
        "\n",
        "        section_counter = 1\n",
        "\n",
        "        for line in lines:\n",
        "            # Check if line is a header\n",
        "            header_match = re.match(r'^(#+)\\s*(.+)$', line)\n",
        "\n",
        "            if header_match:\n",
        "                # Save current section if it has content\n",
        "                if current_section['content'].strip():\n",
        "                    cleaned_content = self._clean_markdown_text(current_section['content'])\n",
        "                    if cleaned_content.strip():\n",
        "                        section_data = {\n",
        "                            'section_number': current_section['section_number'],\n",
        "                            'header': current_section['header'],\n",
        "                            'raw_text': current_section['content'],\n",
        "                            'cleaned_text': cleaned_content,\n",
        "                            'char_count': len(cleaned_content),\n",
        "                            'token_count': len(self.tokenizer.encode(cleaned_content)),\n",
        "                            'level': current_section['level']\n",
        "                        }\n",
        "                        sections_data.append(section_data)\n",
        "\n",
        "                # Start new section\n",
        "                header_level = len(header_match.group(1))\n",
        "                header_text = header_match.group(2).strip()\n",
        "\n",
        "                current_section = {\n",
        "                    'section_number': section_counter,\n",
        "                    'header': header_text,\n",
        "                    'content': '',\n",
        "                    'level': header_level\n",
        "                }\n",
        "                section_counter += 1\n",
        "            else:\n",
        "                # Add line to current section content\n",
        "                current_section['content'] += line + '\\n'\n",
        "\n",
        "        # Don't forget the last section\n",
        "        if current_section['content'].strip():\n",
        "            cleaned_content = self._clean_markdown_text(current_section['content'])\n",
        "            if cleaned_content.strip():\n",
        "                section_data = {\n",
        "                    'section_number': current_section['section_number'],\n",
        "                    'header': current_section['header'],\n",
        "                    'raw_text': current_section['content'],\n",
        "                    'cleaned_text': cleaned_content,\n",
        "                    'char_count': len(cleaned_content),\n",
        "                    'token_count': len(self.tokenizer.encode(cleaned_content)),\n",
        "                    'level': current_section['level']\n",
        "                }\n",
        "                sections_data.append(section_data)\n",
        "\n",
        "        return sections_data\n",
        "\n",
        "    def _clean_markdown_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize markdown text while preserving important formatting\"\"\"\n",
        "\n",
        "        # Remove markdown syntax but preserve structure\n",
        "        # Remove bold/italic markers but keep the text\n",
        "        text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)  # **bold**\n",
        "        text = re.sub(r'\\*(.*?)\\*', r'\\1', text)      # *italic*\n",
        "        text = re.sub(r'__(.*?)__', r'\\1', text)      # __bold__\n",
        "        text = re.sub(r'_(.*?)_', r'\\1', text)        # _italic_\n",
        "\n",
        "        # Clean up code blocks but preserve content\n",
        "        text = re.sub(r'```\\w*\\n(.*?)\\n```', r'\\1', text, flags=re.DOTALL)  # Code blocks\n",
        "        text = re.sub(r'`([^`]+)`', r'\\1', text)      # Inline code\n",
        "\n",
        "        # Remove markdown links but keep text\n",
        "        text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)  # [text](url)\n",
        "\n",
        "        # Remove markdown image syntax\n",
        "        text = re.sub(r'!\\[([^\\]]*)\\]\\([^\\)]+\\)', r'\\1', text)  # ![alt](url)\n",
        "\n",
        "        # Clean up tables - convert to readable format\n",
        "        text = re.sub(r'\\|', ' | ', text)  # Make table separators more readable\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Multiple newlines\n",
        "        text = re.sub(r'[ \\t]+', ' ', text)     # Multiple spaces/tabs\n",
        "\n",
        "        # Remove markdown horizontal rules\n",
        "        text = re.sub(r'^[-=*]{3,}$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "# ==========================================\n",
        "# INTELLIGENT TEXT CHUNKING (Updated for Markdown)\n",
        "# ==========================================\n",
        "\n",
        "class SmartChunker:\n",
        "    \"\"\"Intelligent text chunking with semantic awareness for Markdown\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 800, overlap: int = 150, min_chunk_size: int = 100):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "        self.min_chunk_size = min_chunk_size\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def chunk_documents(self, sections_data: List[Dict], source_file: str) -> List[DocumentChunk]:\n",
        "        \"\"\"Create intelligent chunks from extracted markdown sections\"\"\"\n",
        "\n",
        "        print(f\"ðŸ”ª Chunking document: {source_file}\")\n",
        "\n",
        "        all_chunks = []\n",
        "        chunk_counter = 0\n",
        "\n",
        "        for section_data in tqdm(sections_data, desc=\"Processing sections\"):\n",
        "            section_text = section_data['cleaned_text']\n",
        "            section_number = section_data['section_number']\n",
        "            section_header = section_data['header']\n",
        "\n",
        "            # Create chunks for this section\n",
        "            section_chunks = self._create_semantic_chunks(\n",
        "                text=section_text,\n",
        "                section_title=section_header,\n",
        "                section_number=section_number,\n",
        "                source_file=source_file,\n",
        "                start_chunk_idx=chunk_counter\n",
        "            )\n",
        "\n",
        "            all_chunks.extend(section_chunks)\n",
        "            chunk_counter += len(section_chunks)\n",
        "\n",
        "        print(f\"âœ… Created {len(all_chunks)} chunks\")\n",
        "        return all_chunks\n",
        "\n",
        "    def _create_semantic_chunks(self, text: str, section_title: Optional[str],\n",
        "                              section_number: int, source_file: str,\n",
        "                              start_chunk_idx: int) -> List[DocumentChunk]:\n",
        "        \"\"\"Create semantic chunks from section text\"\"\"\n",
        "\n",
        "        chunks = []\n",
        "\n",
        "        # Split by paragraphs first (better semantic boundaries)\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        if not paragraphs:\n",
        "            return chunks\n",
        "\n",
        "        current_chunk = \"\"\n",
        "        chunk_idx = start_chunk_idx\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            # Check if adding this paragraph would exceed chunk size\n",
        "            potential_chunk = current_chunk + \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
        "            token_count = len(self.tokenizer.encode(potential_chunk))\n",
        "\n",
        "            if token_count <= self.chunk_size:\n",
        "                # Add paragraph to current chunk\n",
        "                current_chunk = potential_chunk\n",
        "            else:\n",
        "                # Current chunk is ready, start new one\n",
        "                if current_chunk:\n",
        "                    chunk = self._create_chunk(\n",
        "                        content=current_chunk,\n",
        "                        source_file=source_file,\n",
        "                        section_number=section_number,\n",
        "                        chunk_index=chunk_idx,\n",
        "                        section_title=section_title\n",
        "                    )\n",
        "                    chunks.append(chunk)\n",
        "                    chunk_idx += 1\n",
        "\n",
        "                # Handle overlap: take last sentences from previous chunk\n",
        "                if chunks and self.overlap > 0:\n",
        "                    overlap_text = self._get_overlap_text(current_chunk, self.overlap)\n",
        "                    current_chunk = overlap_text + \"\\n\\n\" + paragraph\n",
        "                else:\n",
        "                    current_chunk = paragraph\n",
        "\n",
        "        # Add final chunk\n",
        "        if current_chunk and len(current_chunk.strip()) >= self.min_chunk_size:\n",
        "            chunk = self._create_chunk(\n",
        "                content=current_chunk,\n",
        "                source_file=source_file,\n",
        "                section_number=section_number,\n",
        "                chunk_index=chunk_idx,\n",
        "                section_title=section_title\n",
        "            )\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _get_overlap_text(self, text: str, target_tokens: int) -> str:\n",
        "        \"\"\"Get last N tokens from text for overlap\"\"\"\n",
        "\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        overlap_text = \"\"\n",
        "        token_count = 0\n",
        "\n",
        "        # Build overlap from end backwards\n",
        "        for sentence in reversed(sentences):\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            sentence_tokens = len(self.tokenizer.encode(sentence))\n",
        "            if token_count + sentence_tokens <= target_tokens:\n",
        "                overlap_text = sentence + \". \" + overlap_text\n",
        "                token_count += sentence_tokens\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return overlap_text.strip()\n",
        "\n",
        "    def _create_chunk(self, content: str, source_file: str, section_number: int,\n",
        "                     chunk_index: int, section_title: Optional[str]) -> DocumentChunk:\n",
        "        \"\"\"Create a DocumentChunk object\"\"\"\n",
        "\n",
        "        # Generate unique ID\n",
        "        chunk_id = hashlib.md5(\n",
        "            f\"{source_file}_{chunk_index}_{content[:50]}\".encode()\n",
        "        ).hexdigest()[:12]\n",
        "\n",
        "        # Calculate metrics\n",
        "        token_count = len(self.tokenizer.encode(content))\n",
        "        char_count = len(content)\n",
        "\n",
        "        # Determine chunk type based on content and section title\n",
        "        chunk_type = \"content\"\n",
        "        if section_title:\n",
        "            title_lower = section_title.lower()\n",
        "            if any(keyword in title_lower for keyword in ['troubleshoot', 'error', 'issue', 'problem']):\n",
        "                chunk_type = \"troubleshooting\"\n",
        "            elif any(keyword in title_lower for keyword in ['example', 'sample', 'demo']):\n",
        "                chunk_type = \"example\"\n",
        "            elif any(keyword in title_lower for keyword in ['standard', 'guideline', 'rule']):\n",
        "                chunk_type = \"standards\"\n",
        "            elif any(keyword in title_lower for keyword in ['architecture', 'decision', 'adr']):\n",
        "                chunk_type = \"architecture\"\n",
        "            elif any(keyword in title_lower for keyword in ['specification', 'spec', 'program']):\n",
        "                chunk_type = \"specification\"\n",
        "\n",
        "        return DocumentChunk(\n",
        "            id=chunk_id,\n",
        "            content=content,\n",
        "            source_file=source_file,\n",
        "            page_number=section_number,  # Using section number instead of page number\n",
        "            chunk_index=chunk_index,\n",
        "            token_count=token_count,\n",
        "            char_count=char_count,\n",
        "            section_title=section_title,\n",
        "            chunk_type=chunk_type\n",
        "        )\n",
        "\n",
        "# ==========================================\n",
        "# CHUNK QUALITY VALIDATION (Same as before)\n",
        "# ==========================================\n",
        "\n",
        "class ChunkValidator:\n",
        "    \"\"\"Validate chunk quality and filter low-quality chunks\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_chunk(chunk: DocumentChunk) -> Tuple[bool, str]:\n",
        "        \"\"\"Validate chunk quality, return (is_valid, reason)\"\"\"\n",
        "\n",
        "        content = chunk.content.strip()\n",
        "\n",
        "        # Check minimum length\n",
        "        if chunk.char_count < 50:\n",
        "            return False, \"Too short\"\n",
        "\n",
        "        # Check for too many special characters (likely OCR errors)\n",
        "        special_char_ratio = sum(1 for c in content if not c.isalnum() and c not in ' .,!?;:-\\n()[]{}') / len(content)\n",
        "        if special_char_ratio > 0.4:  # Slightly higher threshold for markdown\n",
        "            return False, \"Too many special characters\"\n",
        "\n",
        "        # Check for reasonable word count\n",
        "        words = content.split()\n",
        "        if len(words) < 10:\n",
        "            return False, \"Too few words\"\n",
        "\n",
        "        # Check for excessive repetition\n",
        "        unique_words = set(words)\n",
        "        if len(unique_words) / len(words) < 0.3:\n",
        "            return False, \"Too repetitive\"\n",
        "\n",
        "        # Check for table of contents patterns (usually not useful for RAG)\n",
        "        if re.search(r'\\.{10,}|\\t{5,}|_{10,}', content):\n",
        "            return False, \"Looks like table of contents\"\n",
        "\n",
        "        return True, \"Valid\"\n",
        "\n",
        "    @staticmethod\n",
        "    def filter_chunks(chunks: List[DocumentChunk]) -> Tuple[List[DocumentChunk], Dict[str, int]]:\n",
        "        \"\"\"Filter chunks and return valid ones with statistics\"\"\"\n",
        "\n",
        "        valid_chunks = []\n",
        "        rejection_stats = {}\n",
        "\n",
        "        for chunk in chunks:\n",
        "            is_valid, reason = ChunkValidator.validate_chunk(chunk)\n",
        "\n",
        "            if is_valid:\n",
        "                valid_chunks.append(chunk)\n",
        "            else:\n",
        "                rejection_stats[reason] = rejection_stats.get(reason, 0) + 1\n",
        "\n",
        "        return valid_chunks, rejection_stats\n",
        "\n",
        "# ==========================================\n",
        "# MAIN PROCESSING PIPELINE (Updated for Markdown)\n",
        "# ==========================================\n",
        "\n",
        "def process_uploaded_markdowns(md_files: List[str]) -> List[DocumentChunk]:\n",
        "    \"\"\"Main pipeline to process all uploaded Markdown files\"\"\"\n",
        "\n",
        "    print(\"\\nðŸ”„ Starting Markdown Document Processing Pipeline\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Initialize processors\n",
        "    md_processor = MarkdownProcessor()\n",
        "    chunker = SmartChunker(chunk_size=800, overlap=150)\n",
        "\n",
        "    all_chunks = []\n",
        "    processing_stats = {}\n",
        "\n",
        "    for md_path in md_files:\n",
        "        filename = md_path.split('/')[-1]\n",
        "        print(f\"\\nðŸ“š Processing: {filename}\")\n",
        "\n",
        "        # Extract text from Markdown\n",
        "        sections_data = md_processor.extract_text_from_markdown(md_path)\n",
        "\n",
        "        if not sections_data:\n",
        "            print(f\"âš ï¸ No text extracted from {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Create chunks\n",
        "        chunks = chunker.chunk_documents(sections_data, filename)\n",
        "\n",
        "        # Validate chunks\n",
        "        valid_chunks, rejection_stats = ChunkValidator.filter_chunks(chunks)\n",
        "\n",
        "        # Update statistics\n",
        "        processing_stats[filename] = {\n",
        "            'sections': len(sections_data),\n",
        "            'total_chunks': len(chunks),\n",
        "            'valid_chunks': len(valid_chunks),\n",
        "            'rejected': len(chunks) - len(valid_chunks),\n",
        "            'rejection_reasons': rejection_stats\n",
        "        }\n",
        "\n",
        "        all_chunks.extend(valid_chunks)\n",
        "\n",
        "        print(f\"âœ… {filename}: {len(valid_chunks)} valid chunks from {len(chunks)} total\")\n",
        "\n",
        "    # Display final statistics\n",
        "    print(\"\\nðŸ“Š PROCESSING SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    total_sections = sum(stats['sections'] for stats in processing_stats.values())\n",
        "    total_valid_chunks = len(all_chunks)\n",
        "    total_rejected = sum(stats['rejected'] for stats in processing_stats.values())\n",
        "\n",
        "    print(f\"ðŸ“„ Total sections processed: {total_sections}\")\n",
        "    print(f\"âœ… Valid chunks created: {total_valid_chunks}\")\n",
        "    print(f\"âŒ Chunks rejected: {total_rejected}\")\n",
        "\n",
        "    if total_rejected > 0:\n",
        "        print(\"\\nðŸ” Rejection reasons:\")\n",
        "        all_rejection_reasons = {}\n",
        "        for stats in processing_stats.values():\n",
        "            for reason, count in stats['rejection_reasons'].items():\n",
        "                all_rejection_reasons[reason] = all_rejection_reasons.get(reason, 0) + count\n",
        "\n",
        "        for reason, count in all_rejection_reasons.items():\n",
        "            print(f\"  â€¢ {reason}: {count}\")\n",
        "\n",
        "    # Sample some chunks for review\n",
        "    if all_chunks:\n",
        "        print(f\"\\nðŸ“‹ Sample chunks:\")\n",
        "        for i, chunk in enumerate(all_chunks[:3]):\n",
        "            print(f\"\\nChunk {i+1} (ID: {chunk.id}):\")\n",
        "            print(f\"Source: {chunk.source_file}, Section: {chunk.page_number}\")\n",
        "            print(f\"Section Title: {chunk.section_title or 'N/A'}\")\n",
        "            print(f\"Content preview: {chunk.content[:200]}...\")\n",
        "            print(f\"Tokens: {chunk.token_count}, Type: {chunk.chunk_type}\")\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"ðŸŽ‰ Markdown document processing complete! Ready for Phase 1C (Embedding & Storage)\")\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION (Updated for Markdown)\n",
        "# ==========================================\n",
        "\n",
        "# Process the uploaded Markdown files (assuming md_files variable exists from Phase 1A)\n",
        "try:\n",
        "    processed_chunks = process_uploaded_markdowns(md_files)\n",
        "    print(f\"\\nâœ… Phase 1B Complete: {len(processed_chunks)} chunks ready for embedding\")\n",
        "\n",
        "    # Store for next phase\n",
        "    phase1b_output = {\n",
        "        'chunks': processed_chunks,\n",
        "        'total_chunks': len(processed_chunks),\n",
        "        'ready_for_embedding': True\n",
        "    }\n",
        "\n",
        "except NameError:\n",
        "    print(\"âŒ md_files not found. Please run Phase 1A first to upload Markdown files.\")\n",
        "    print(\"ðŸ“‹ Expected variable: md_files (list of Markdown file paths)\")\n",
        "\n",
        "    # For testing, you can manually set:\n",
        "    # md_files = ['/content/your_migration_guide.md']  # Add your MD paths here\n",
        "    # processed_chunks = process_uploaded_markdowns(md_files)"
      ],
      "metadata": {
        "id": "MHwDnTnwlL9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating embeddings and storing in vector database"
      ],
      "metadata": {
        "id": "w-vk3fQDZqkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"ðŸ§  Generating embeddings and storing in vector database...\")\n",
        "\n",
        "# ==========================================\n",
        "# EMBEDDING GENERATION\n",
        "# ==========================================\n",
        "\n",
        "class EmbeddingGenerator:\n",
        "    \"\"\"Generate and manage embeddings for document chunks\"\"\"\n",
        "\n",
        "    def __init__(self, model, batch_size: int = 32):\n",
        "        self.model = model\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_cache = {}\n",
        "\n",
        "    def generate_embeddings(self, chunks: List[DocumentChunk]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Generate embeddings for all chunks with batching\"\"\"\n",
        "\n",
        "        print(f\"ðŸ”„ Generating embeddings for {len(chunks)} chunks...\")\n",
        "\n",
        "        # Prepare texts for embedding\n",
        "        texts = []\n",
        "        chunk_ids = []\n",
        "\n",
        "        for chunk in chunks:\n",
        "            # Create rich text for embedding (includes context)\n",
        "            embedding_text = self._prepare_embedding_text(chunk)\n",
        "            texts.append(embedding_text)\n",
        "            chunk_ids.append(chunk.id)\n",
        "\n",
        "        # Generate embeddings in batches\n",
        "        all_embeddings = {}\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), self.batch_size), desc=\"Generating embeddings\"):\n",
        "            batch_texts = texts[i:i + self.batch_size]\n",
        "            batch_ids = chunk_ids[i:i + self.batch_size]\n",
        "\n",
        "            # Generate embeddings for this batch\n",
        "            batch_embeddings = self.model.encode(\n",
        "                batch_texts,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "\n",
        "            # Store embeddings\n",
        "            for chunk_id, embedding in zip(batch_ids, batch_embeddings):\n",
        "                all_embeddings[chunk_id] = embedding\n",
        "\n",
        "        print(f\"âœ… Generated {len(all_embeddings)} embeddings\")\n",
        "        return all_embeddings\n",
        "\n",
        "    def _prepare_embedding_text(self, chunk: DocumentChunk) -> str:\n",
        "        \"\"\"Prepare rich text for embedding generation with migration-specific context\"\"\"\n",
        "\n",
        "        # Start with main content\n",
        "        embedding_text = chunk.content\n",
        "\n",
        "        # Add section context if available\n",
        "        if chunk.section_title:\n",
        "            embedding_text = f\"Section: {chunk.section_title}\\n\\n{embedding_text}\"\n",
        "\n",
        "        # Add document type and migration context\n",
        "        source_context = f\"Source: {chunk.source_file}\"\n",
        "\n",
        "        # Add chunk type for better retrieval\n",
        "        if chunk.chunk_type != \"content\":\n",
        "            source_context += f\" (Type: {chunk.chunk_type})\"\n",
        "\n",
        "        # Add migration domain context\n",
        "        migration_context = \"Domain: Mainframe to Java Migration\"\n",
        "\n",
        "        # Determine document category from filename\n",
        "        filename_lower = chunk.source_file.lower()\n",
        "        if 'playbook' in filename_lower or 'migration' in filename_lower:\n",
        "            migration_context += \" - Migration Guide\"\n",
        "        elif 'troubleshoot' in filename_lower:\n",
        "            migration_context += \" - Troubleshooting\"\n",
        "        elif 'standard' in filename_lower or 'java' in filename_lower:\n",
        "            migration_context += \" - Development Standards\"\n",
        "        elif 'architecture' in filename_lower or 'adr' in filename_lower:\n",
        "            migration_context += \" - Architecture Decisions\"\n",
        "        elif 'spec' in filename_lower or 'cobol' in filename_lower:\n",
        "            migration_context += \" - Technical Specifications\"\n",
        "\n",
        "        embedding_text = f\"{source_context}\\n{migration_context}\\n\\n{embedding_text}\"\n",
        "\n",
        "        return embedding_text\n",
        "\n",
        "# ==========================================\n",
        "# VECTOR DATABASE STORAGE\n",
        "# ==========================================\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"Manage ChromaDB vector storage operations\"\"\"\n",
        "\n",
        "    def __init__(self, collection):\n",
        "        self.collection = collection\n",
        "        self.stored_count = 0\n",
        "\n",
        "    def store_chunks_with_embeddings(self, chunks: List[DocumentChunk],\n",
        "                                   embeddings: Dict[str, np.ndarray]) -> bool:\n",
        "        \"\"\"Store chunks and their embeddings in ChromaDB\"\"\"\n",
        "\n",
        "        print(f\"ðŸ’¾ Storing {len(chunks)} chunks in vector database...\")\n",
        "\n",
        "        try:\n",
        "            # Prepare data for ChromaDB\n",
        "            ids = []\n",
        "            documents = []\n",
        "            metadatas = []\n",
        "            embeddings_list = []\n",
        "\n",
        "            for chunk in tqdm(chunks, desc=\"Preparing for storage\"):\n",
        "                if chunk.id in embeddings:\n",
        "                    ids.append(chunk.id)\n",
        "                    documents.append(chunk.content)\n",
        "                    metadatas.append(self._prepare_metadata(chunk))\n",
        "                    embeddings_list.append(embeddings[chunk.id].tolist())\n",
        "\n",
        "            # Store in ChromaDB (batch operation)\n",
        "            self.collection.add(\n",
        "                ids=ids,\n",
        "                documents=documents,\n",
        "                metadatas=metadatas,\n",
        "                embeddings=embeddings_list\n",
        "            )\n",
        "\n",
        "            self.stored_count = len(ids)\n",
        "            print(f\"âœ… Successfully stored {self.stored_count} chunks in vector database\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error storing chunks: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _prepare_metadata(self, chunk: DocumentChunk) -> Dict[str, Any]:\n",
        "        \"\"\"Prepare enhanced metadata for ChromaDB storage\"\"\"\n",
        "\n",
        "        metadata = {\n",
        "            'source_file': chunk.source_file,\n",
        "            'section_number': chunk.page_number,  # Using section number instead of page\n",
        "            'chunk_index': chunk.chunk_index,\n",
        "            'token_count': chunk.token_count,\n",
        "            'char_count': chunk.char_count,\n",
        "            'chunk_type': chunk.chunk_type\n",
        "        }\n",
        "\n",
        "        # Add section title if available\n",
        "        if chunk.section_title:\n",
        "            metadata['section_title'] = chunk.section_title\n",
        "\n",
        "        # Add document category based on filename\n",
        "        filename_lower = chunk.source_file.lower()\n",
        "        if 'playbook' in filename_lower:\n",
        "            metadata['doc_category'] = 'migration_guide'\n",
        "        elif 'troubleshoot' in filename_lower:\n",
        "            metadata['doc_category'] = 'troubleshooting'\n",
        "        elif 'standard' in filename_lower or 'java' in filename_lower:\n",
        "            metadata['doc_category'] = 'development_standards'\n",
        "        elif 'architecture' in filename_lower or 'adr' in filename_lower:\n",
        "            metadata['doc_category'] = 'architecture'\n",
        "        elif 'spec' in filename_lower or 'cobol' in filename_lower:\n",
        "            metadata['doc_category'] = 'specification'\n",
        "        else:\n",
        "            metadata['doc_category'] = 'general'\n",
        "\n",
        "        # Add content indicators\n",
        "        content_lower = chunk.content.lower()\n",
        "        metadata['has_code'] = bool(any(lang in content_lower for lang in ['java', 'cobol', 'sql', 'yaml', 'xml']))\n",
        "        metadata['has_error_info'] = bool(any(term in content_lower for term in ['error', 'exception', 'failed', 'issue']))\n",
        "        metadata['migration_phase'] = 'unknown'\n",
        "\n",
        "        # Determine migration phase\n",
        "        if any(term in content_lower for term in ['planning', 'assessment', 'analysis']):\n",
        "            metadata['migration_phase'] = 'planning'\n",
        "        elif any(term in content_lower for term in ['conversion', 'transform', 'migrate']):\n",
        "            metadata['migration_phase'] = 'execution'\n",
        "        elif any(term in content_lower for term in ['test', 'validate', 'verify']):\n",
        "            metadata['migration_phase'] = 'testing'\n",
        "        elif any(term in content_lower for term in ['deploy', 'production', 'cutover']):\n",
        "            metadata['migration_phase'] = 'deployment'\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def get_collection_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get statistics about the vector database\"\"\"\n",
        "\n",
        "        try:\n",
        "            count = self.collection.count()\n",
        "            sample_metadata = None\n",
        "\n",
        "            if count > 0:\n",
        "                # Get a sample to check metadata structure\n",
        "                sample = self.collection.peek(limit=1)\n",
        "                if sample['metadatas']:\n",
        "                    sample_metadata = sample['metadatas'][0]\n",
        "\n",
        "            stats = {\n",
        "                'total_documents': count,\n",
        "                'collection_name': self.collection.name,\n",
        "                'sample_metadata_keys': list(sample_metadata.keys()) if sample_metadata else [],\n",
        "                'storage_success': count > 0\n",
        "            }\n",
        "\n",
        "            return stats\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error getting collection stats: {e}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "# ==========================================\n",
        "# RETRIEVAL SYSTEM\n",
        "# ==========================================\n",
        "\n",
        "class RetrievalSystem:\n",
        "    \"\"\"Handle similarity search and retrieval operations for migration content\"\"\"\n",
        "\n",
        "    def __init__(self, collection, embedding_model, top_k: int = 5):\n",
        "        self.collection = collection\n",
        "        self.embedding_model = embedding_model\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def search(self, query: str, top_k: Optional[int] = None, filter_metadata: Dict = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search for relevant chunks using semantic similarity\"\"\"\n",
        "\n",
        "        search_k = top_k or self.top_k\n",
        "\n",
        "        try:\n",
        "            # Enhance query with migration context\n",
        "            enhanced_query = self._enhance_query(query)\n",
        "\n",
        "            # Generate query embedding\n",
        "            query_embedding = self.embedding_model.encode([enhanced_query])\n",
        "\n",
        "            # Prepare search parameters\n",
        "            search_params = {\n",
        "                'query_embeddings': query_embedding.tolist(),\n",
        "                'n_results': search_k,\n",
        "                'include': ['documents', 'metadatas', 'distances']\n",
        "            }\n",
        "\n",
        "            # Add metadata filtering if provided\n",
        "            if filter_metadata:\n",
        "                search_params['where'] = filter_metadata\n",
        "\n",
        "            # Search in ChromaDB\n",
        "            results = self.collection.query(**search_params)\n",
        "\n",
        "            # Format results\n",
        "            formatted_results = []\n",
        "\n",
        "            if results['documents'] and results['documents'][0]:\n",
        "                for i in range(len(results['documents'][0])):\n",
        "                    result = {\n",
        "                        'id': results['ids'][0][i],\n",
        "                        'content': results['documents'][0][i],\n",
        "                        'metadata': results['metadatas'][0][i],\n",
        "                        'similarity_score': max(0, 1 - (results['distances'][0][i] / 2)),\n",
        "                        'distance': results['distances'][0][i]\n",
        "                    }\n",
        "                    formatted_results.append(result)\n",
        "\n",
        "            return formatted_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Search error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _enhance_query(self, query: str) -> str:\n",
        "        \"\"\"Enhance queries with migration-specific context\"\"\"\n",
        "\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Add context for common migration terms\n",
        "        if any(term in query_lower for term in ['decimal', 'precision', 'comp-3']):\n",
        "            return f\"COBOL to Java decimal conversion: {query}\"\n",
        "        elif any(term in query_lower for term in ['batch', 'jcl']):\n",
        "            return f\"Mainframe batch to Spring Batch: {query}\"\n",
        "        elif any(term in query_lower for term in ['error', 'issue', 'problem']):\n",
        "            return f\"Migration troubleshooting: {query}\"\n",
        "        elif any(term in query_lower for term in ['spring', 'java']):\n",
        "            return f\"Java development standards migration: {query}\"\n",
        "        elif any(term in query_lower for term in ['aws', 'cloud']):\n",
        "            return f\"AWS mainframe modernization: {query}\"\n",
        "        else:\n",
        "            return f\"Mainframe to Java migration: {query}\"\n",
        "\n",
        "    def search_by_category(self, query: str, category: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search within a specific document category\"\"\"\n",
        "\n",
        "        return self.search(\n",
        "            query=query,\n",
        "            top_k=top_k,\n",
        "            filter_metadata={'doc_category': category}\n",
        "        )\n",
        "\n",
        "    def test_retrieval(self, test_queries: List[str] = None) -> None:\n",
        "        \"\"\"Test retrieval system with migration-specific sample queries\"\"\"\n",
        "\n",
        "        if not test_queries:\n",
        "            test_queries = [\n",
        "                \"How do you convert COBOL COMP-3 fields to Java BigDecimal?\",\n",
        "                \"What are the steps for AWS Transform migration?\",\n",
        "                \"How to troubleshoot decimal precision errors?\",\n",
        "                \"What Spring Boot patterns for financial applications?\",\n",
        "                \"Why choose PostgreSQL over Oracle for migration?\",\n",
        "                \"How to handle CICS transaction context in Java?\",\n",
        "                \"What testing strategy for COBOL to Java migration?\",\n",
        "                \"How to convert JCL batch jobs to Spring Batch?\",\n",
        "                \"What are common overdraft calculation errors?\",\n",
        "                \"How to fix connection timeout issues in Java?\"\n",
        "            ]\n",
        "\n",
        "        print(f\"\\nðŸ” Testing retrieval with {len(test_queries)} migration-specific queries...\")\n",
        "\n",
        "        for i, query in enumerate(test_queries, 1):\n",
        "            print(f\"\\n--- Test Query {i}: '{query}' ---\")\n",
        "\n",
        "            results = self.search(query, top_k=3)\n",
        "\n",
        "            if results:\n",
        "                for j, result in enumerate(results, 1):\n",
        "                    similarity = result['similarity_score']\n",
        "                    source = result['metadata'].get('source_file', 'Unknown')\n",
        "                    section = result['metadata'].get('section_number', 'N/A')\n",
        "                    section_title = result['metadata'].get('section_title', 'N/A')\n",
        "                    doc_category = result['metadata'].get('doc_category', 'N/A')\n",
        "\n",
        "                    print(f\"  {j}. Score: {similarity:.3f} | {source} (Section {section})\")\n",
        "                    print(f\"     Category: {doc_category} | Section: {section_title}\")\n",
        "                    print(f\"     Preview: {result['content'][:100]}...\")\n",
        "            else:\n",
        "                print(\"  No results found\")\n",
        "\n",
        "    def test_category_search(self) -> None:\n",
        "        \"\"\"Test category-specific searches\"\"\"\n",
        "\n",
        "        print(f\"\\nðŸ·ï¸ Testing category-specific searches...\")\n",
        "\n",
        "        category_tests = [\n",
        "            (\"troubleshooting\", \"How to fix decimal precision errors?\"),\n",
        "            (\"development_standards\", \"What BigDecimal patterns should I use?\"),\n",
        "            (\"architecture\", \"Why was PostgreSQL chosen over Oracle?\"),\n",
        "            (\"specification\", \"What business rules does ACCVAL01 implement?\"),\n",
        "            (\"migration_guide\", \"What were the performance improvements?\")\n",
        "        ]\n",
        "\n",
        "        for category, query in category_tests:\n",
        "            print(f\"\\n--- Category: {category} | Query: '{query}' ---\")\n",
        "            results = self.search_by_category(query, category, top_k=2)\n",
        "\n",
        "            if results:\n",
        "                for j, result in enumerate(results, 1):\n",
        "                    similarity = result['similarity_score']\n",
        "                    source = result['metadata'].get('source_file', 'Unknown')\n",
        "                    print(f\"  {j}. Score: {similarity:.3f} | {source}\")\n",
        "                    print(f\"     Preview: {result['content'][:80]}...\")\n",
        "            else:\n",
        "                print(\"  No results found in this category\")\n",
        "\n",
        "# ==========================================\n",
        "# MAIN EXECUTION PIPELINE\n",
        "# ==========================================\n",
        "\n",
        "def setup_vector_database(chunks: List[DocumentChunk], embedding_model, collection):\n",
        "    \"\"\"Complete pipeline to set up vector database with embeddings for migration content\"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Setting up migration knowledge vector database...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Generate embeddings\n",
        "    embedding_generator = EmbeddingGenerator(embedding_model, batch_size=32)\n",
        "    embeddings = embedding_generator.generate_embeddings(chunks)\n",
        "\n",
        "    # Step 2: Store in vector database\n",
        "    vector_store = VectorStore(collection)\n",
        "    storage_success = vector_store.store_chunks_with_embeddings(chunks, embeddings)\n",
        "\n",
        "    if not storage_success:\n",
        "        print(\"âŒ Failed to store chunks in vector database\")\n",
        "        return None\n",
        "\n",
        "    # Step 3: Set up retrieval system\n",
        "    retrieval_system = RetrievalSystem(collection, embedding_model)\n",
        "\n",
        "    # Step 4: Get database statistics\n",
        "    stats = vector_store.get_collection_stats()\n",
        "\n",
        "    print(\"\\nðŸ“Š MIGRATION KNOWLEDGE BASE STATISTICS\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Total documents stored: {stats.get('total_documents', 0)}\")\n",
        "    print(f\"Collection name: {stats.get('collection_name', 'N/A')}\")\n",
        "    print(f\"Metadata fields: {', '.join(stats.get('sample_metadata_keys', []))}\")\n",
        "\n",
        "    # Step 5: Test retrieval systems\n",
        "    retrieval_system.test_retrieval()\n",
        "    retrieval_system.test_category_search()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… Migration knowledge vector database setup complete!\")\n",
        "    print(\"ðŸ” Retrieval system ready for mainframe migration queries\")\n",
        "    print(\"ðŸŽ¯ Ready for agentic RAG pipeline with Tavily integration!\")\n",
        "\n",
        "    return retrieval_system\n",
        "\n",
        "# Execute the pipeline\n",
        "try:\n",
        "    # Use chunks from previous phase\n",
        "    retrieval_system = setup_vector_database(\n",
        "        chunks=processed_chunks,\n",
        "        embedding_model=embedding_model,\n",
        "        collection=collection\n",
        "    )\n",
        "\n",
        "    if retrieval_system:\n",
        "        print(f\"\\nðŸŽ‰ Success! Migration knowledge base contains {collection.count()} documents\")\n",
        "\n",
        "        # Store for next phase\n",
        "        vector_db_setup = {\n",
        "            'retrieval_system': retrieval_system,\n",
        "            'collection': collection,\n",
        "            'embedding_model': embedding_model,\n",
        "            'total_documents': collection.count(),\n",
        "            'ready_for_generation': True\n",
        "        }\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"âŒ Missing variable: {e}\")\n",
        "    print(\"ðŸ“‹ Required variables from previous phases:\")\n",
        "    print(\"  â€¢ processed_chunks (from markdown document processing)\")\n",
        "    print(\"  â€¢ embedding_model (from setup)\")\n",
        "    print(\"  â€¢ collection (from ChromaDB setup)\")\n",
        "    print(\"\\nðŸ’¡ Make sure to run previous phases first!\")\n",
        "\n",
        "# Enhanced query testing functions\n",
        "def test_migration_query(query: str, category: str = None):\n",
        "    \"\"\"Test a single migration query against the vector database\"\"\"\n",
        "    if 'retrieval_system' in globals():\n",
        "        if category:\n",
        "            results = retrieval_system.search_by_category(query, category)\n",
        "            print(f\"\\nCategory Search: '{category}' | Query: '{query}'\")\n",
        "        else:\n",
        "            results = retrieval_system.search(query)\n",
        "            print(f\"\\nGeneral Search: '{query}'\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"{i}. Score: {result['similarity_score']:.3f}\")\n",
        "            print(f\"   Source: {result['metadata']['source_file']}\")\n",
        "            print(f\"   Category: {result['metadata'].get('doc_category', 'N/A')}\")\n",
        "            print(f\"   Section: {result['metadata'].get('section_title', 'N/A')}\")\n",
        "            print(f\"   Content: {result['content'][:120]}...\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"âŒ Retrieval system not available. Run the setup first.\")\n",
        "\n",
        "def show_database_categories():\n",
        "    \"\"\"Show available document categories in the database\"\"\"\n",
        "    if 'collection' in globals():\n",
        "        try:\n",
        "            # Get all metadata to show categories\n",
        "            sample_data = collection.peek(limit=50)\n",
        "            categories = set()\n",
        "\n",
        "            for metadata in sample_data['metadatas']:\n",
        "                if 'doc_category' in metadata:\n",
        "                    categories.add(metadata['doc_category'])\n",
        "\n",
        "            print(\"\\nðŸ“‹ Available Document Categories:\")\n",
        "            for category in sorted(categories):\n",
        "                print(f\"  â€¢ {category}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error retrieving categories: {e}\")\n",
        "    else:\n",
        "        print(\"âŒ Database not available.\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Available functions:\")\n",
        "print(\"  â€¢ test_migration_query('your question here') - Test general search\")\n",
        "print(\"  â€¢ test_migration_query('question', 'category') - Test category search\")\n",
        "print(\"  â€¢ show_database_categories() - Show available categories\")"
      ],
      "metadata": {
        "id": "enNLbBSHVhOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG system for mainframe migration queries with confidence scoring"
      ],
      "metadata": {
        "id": "S0qhXVskZuZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "print(\"ðŸš€ Mainframe Migration RAG System with Enhanced Confidence Scoring\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# ENHANCED RAG SYSTEM WITH CONFIDENCE SCORING\n",
        "# ==========================================\n",
        "\n",
        "class MainframeMigrationRAG:\n",
        "    \"\"\"RAG system with comprehensive confidence scoring\"\"\"\n",
        "\n",
        "    def __init__(self, retrieval_system, embedding_model):\n",
        "        self.retrieval_system = retrieval_system\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "        # Confidence thresholds\n",
        "        self.confidence_thresholds = {\n",
        "            'excellent': 85,\n",
        "            'high': 70,\n",
        "            'medium': 50,\n",
        "            'low': 30,\n",
        "            'very_low': 15\n",
        "        }\n",
        "\n",
        "        # System prompt template\n",
        "        self.system_prompt = \"\"\"You are a mainframe to Java migration assistant. You help users with COBOL to Java conversion questions based on the provided migration documentation.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Answer based ONLY on the provided context from migration documentation\n",
        "- Be specific and practical in your responses\n",
        "- Include relevant code examples (COBOL and Java) when applicable\n",
        "- If the context doesn't contain enough information, say so clearly\n",
        "- Cite the source material when possible\n",
        "- Format your response clearly with examples when helpful\n",
        "- Focus on practical migration guidance and troubleshooting\n",
        "\n",
        "CONTEXT FROM MIGRATION DOCUMENTATION:\n",
        "{context}\n",
        "\n",
        "USER QUESTION: {query}\n",
        "\n",
        "Please provide a helpful, accurate answer based on the migration context above.\"\"\"\n",
        "\n",
        "    def ask(self, query: str, show_details: bool = False, top_k: int = 5) -> str:\n",
        "        \"\"\"\n",
        "        Ask a migration question and get an answer with confidence scoring\n",
        "\n",
        "        Args:\n",
        "            query: Your migration question\n",
        "            show_details: Set to True to see internal processing details\n",
        "            top_k: Number of relevant chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            String answer to your question\n",
        "        \"\"\"\n",
        "\n",
        "        if show_details:\n",
        "            print(f\"\\nðŸ” PROCESSING: '{query}'\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "        # Step 1: Retrieve relevant chunks\n",
        "        if show_details:\n",
        "            print(\"ðŸ“š Step 1: Searching migration knowledge base...\")\n",
        "\n",
        "        retrieved_chunks = self.retrieval_system.search(query, top_k=top_k)\n",
        "\n",
        "        if not retrieved_chunks:\n",
        "            answer = \"I couldn't find any relevant information in the migration documentation for your question.\"\n",
        "            if show_details:\n",
        "                print(\"âŒ No relevant chunks found\")\n",
        "            self._show_clean_result(query, answer, 0, [], \"No relevant information found\")\n",
        "            return answer\n",
        "\n",
        "        # Step 2: Show retrieval analysis (if details requested)\n",
        "        if show_details:\n",
        "            self._show_retrieval_details(retrieved_chunks)\n",
        "\n",
        "        # Step 3: Filter and assemble context\n",
        "        context, filtered_chunks = self._assemble_context(retrieved_chunks, show_details)\n",
        "\n",
        "        # Step 4: Generate response\n",
        "        if show_details:\n",
        "            print(\"ðŸ¤– Step 4: Generating AI response...\")\n",
        "\n",
        "        try:\n",
        "            prompt = self.system_prompt.format(context=context, query=query)\n",
        "\n",
        "            if show_details:\n",
        "                print(f\"   ðŸ“ Context length: {len(context)} chars\")\n",
        "                print(f\"   ðŸ“¨ Full prompt length: {len(prompt)} chars\")\n",
        "\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=1500,\n",
        "                temperature=0.1,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            answer = response.choices[0].message.content.strip()\n",
        "\n",
        "            if show_details:\n",
        "                print(f\"âœ… Response generated ({len(answer)} characters)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error calling OpenAI API: {e}\"\n",
        "            if show_details:\n",
        "                print(f\"âŒ {error_msg}\")\n",
        "            error_answer = f\"Sorry, I encountered an error: {error_msg}\"\n",
        "            self._show_clean_result(query, error_answer, 0, [], \"API Error\")\n",
        "            return error_answer\n",
        "\n",
        "        # Step 5: Calculate comprehensive confidence score\n",
        "        confidence_score, confidence_breakdown = self._calculate_comprehensive_confidence(\n",
        "            query, filtered_chunks, answer, context\n",
        "        )\n",
        "\n",
        "        # Step 6: Show results\n",
        "        if show_details:\n",
        "            self._show_detailed_confidence_analysis(confidence_score, confidence_breakdown)\n",
        "            self._show_final_details(answer, filtered_chunks, confidence_score)\n",
        "        else:\n",
        "            confidence_label = self._score_to_confidence_label(confidence_score)\n",
        "            self._show_clean_result(query, answer, confidence_score, filtered_chunks, confidence_breakdown['primary_reason'])\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def _calculate_comprehensive_confidence(self, query: str, chunks: List[Dict], answer: str, context: str) -> tuple[int, Dict]:\n",
        "        \"\"\"\n",
        "        Calculate comprehensive confidence score (0-100) with detailed breakdown\n",
        "        \"\"\"\n",
        "\n",
        "        confidence_factors = {}\n",
        "        total_score = 0\n",
        "\n",
        "        # 1. RETRIEVAL QUALITY FACTOR (25 points max)\n",
        "        if chunks:\n",
        "            max_similarity = max(chunk['similarity_score'] for chunk in chunks)\n",
        "            avg_similarity = sum(chunk['similarity_score'] for chunk in chunks) / len(chunks)\n",
        "            high_quality_chunks = sum(1 for chunk in chunks if chunk['similarity_score'] >= 0.7)\n",
        "\n",
        "            retrieval_score = min(25, int(\n",
        "                (max_similarity * 15) +  # Best match contributes 15 points max\n",
        "                (avg_similarity * 5) +   # Average quality contributes 5 points max\n",
        "                (high_quality_chunks * 2.5)  # Each high-quality chunk adds 2.5 points\n",
        "            ))\n",
        "\n",
        "            confidence_factors['retrieval_quality'] = {\n",
        "                'score': retrieval_score,\n",
        "                'max_similarity': max_similarity,\n",
        "                'avg_similarity': avg_similarity,\n",
        "                'high_quality_chunks': high_quality_chunks\n",
        "            }\n",
        "        else:\n",
        "            retrieval_score = 0\n",
        "            confidence_factors['retrieval_quality'] = {'score': 0, 'reason': 'No chunks retrieved'}\n",
        "\n",
        "        total_score += retrieval_score\n",
        "\n",
        "        # 2. ANSWER COMPLETENESS FACTOR (25 points max)\n",
        "        answer_lower = answer.lower()\n",
        "\n",
        "        # Negative indicators\n",
        "        insufficient_phrases = [\n",
        "            \"i couldn't find\", \"no information\", \"not mentioned\", \"not discussed\",\n",
        "            \"does not provide\", \"cannot find\", \"not available\", \"insufficient information\",\n",
        "            \"based on the provided context, i cannot\", \"the document does not contain\"\n",
        "        ]\n",
        "\n",
        "        has_insufficient_info = any(phrase in answer_lower for phrase in insufficient_phrases)\n",
        "\n",
        "        if has_insufficient_info:\n",
        "            completeness_score = 5  # Very low score for incomplete answers\n",
        "            completeness_reason = \"Answer indicates insufficient information\"\n",
        "        else:\n",
        "            # Positive indicators\n",
        "            answer_length = len(answer)\n",
        "            has_examples = bool(re.search(r'(example|for instance|such as)', answer_lower))\n",
        "            has_code = bool(re.search(r'(cobol|java|```|public class)', answer_lower))\n",
        "            has_specific_details = bool(re.search(r'(bigdecimal|comp-3|spring|aws)', answer_lower))\n",
        "\n",
        "            completeness_score = min(25, int(\n",
        "                (min(answer_length / 50, 10)) +  # Length factor (up to 10 points)\n",
        "                (7 if has_examples else 0) +     # Examples add 7 points\n",
        "                (5 if has_code else 0) +         # Code examples add 5 points\n",
        "                (3 if has_specific_details else 0)  # Technical details add 3 points\n",
        "            ))\n",
        "\n",
        "            completeness_reason = f\"Answer is {'comprehensive' if completeness_score > 20 else 'adequate' if completeness_score > 15 else 'basic'}\"\n",
        "\n",
        "        confidence_factors['answer_completeness'] = {\n",
        "            'score': completeness_score,\n",
        "            'has_insufficient_info': has_insufficient_info,\n",
        "            'answer_length': len(answer),\n",
        "            'has_examples': has_examples if not has_insufficient_info else False,\n",
        "            'has_code': has_code if not has_insufficient_info else False,\n",
        "            'reason': completeness_reason\n",
        "        }\n",
        "\n",
        "        total_score += completeness_score\n",
        "\n",
        "        # 3. CONTEXT RELEVANCE FACTOR (25 points max)\n",
        "        query_words = set(query.lower().split())\n",
        "        context_words = set(context.lower().split())\n",
        "        answer_words = set(answer.lower().split())\n",
        "\n",
        "        # Query-context alignment\n",
        "        query_context_overlap = len(query_words.intersection(context_words)) / max(len(query_words), 1)\n",
        "\n",
        "        # Context-answer alignment\n",
        "        context_answer_overlap = len(context_words.intersection(answer_words)) / max(len(context_words), 1)\n",
        "\n",
        "        relevance_score = min(25, int(\n",
        "            (query_context_overlap * 15) +    # Query-context alignment (15 points max)\n",
        "            (context_answer_overlap * 10)     # Context-answer alignment (10 points max)\n",
        "        ))\n",
        "\n",
        "        confidence_factors['context_relevance'] = {\n",
        "            'score': relevance_score,\n",
        "            'query_context_overlap': query_context_overlap,\n",
        "            'context_answer_overlap': context_answer_overlap\n",
        "        }\n",
        "\n",
        "        total_score += relevance_score\n",
        "\n",
        "        # 4. SOURCE DIVERSITY FACTOR (15 points max)\n",
        "        if chunks:\n",
        "            unique_sources = set(chunk['metadata'].get('source_file', 'unknown') for chunk in chunks)\n",
        "            unique_doc_types = set(chunk['metadata'].get('doc_category', 'unknown') for chunk in chunks)\n",
        "\n",
        "            diversity_score = min(15, int(\n",
        "                (len(unique_sources) * 4) +      # Each unique source adds 4 points\n",
        "                (len(unique_doc_types) * 3)      # Each unique doc type adds 3 points\n",
        "            ))\n",
        "\n",
        "            confidence_factors['source_diversity'] = {\n",
        "                'score': diversity_score,\n",
        "                'unique_sources': len(unique_sources),\n",
        "                'unique_doc_types': len(unique_doc_types),\n",
        "                'sources': list(unique_sources)\n",
        "            }\n",
        "        else:\n",
        "            diversity_score = 0\n",
        "            confidence_factors['source_diversity'] = {'score': 0}\n",
        "\n",
        "        total_score += diversity_score\n",
        "\n",
        "        # 5. TECHNICAL SPECIFICITY FACTOR (10 points max)\n",
        "        technical_terms = [\n",
        "            'cobol', 'java', 'bigdecimal', 'comp-3', 'spring', 'aws', 'migration',\n",
        "            'transform', 'mainframe', 'jcl', 'cics', 'batch', 'postgresql', 'oracle'\n",
        "        ]\n",
        "\n",
        "        technical_matches = sum(1 for term in technical_terms if term in answer_lower)\n",
        "        specificity_score = min(10, technical_matches * 2)\n",
        "\n",
        "        confidence_factors['technical_specificity'] = {\n",
        "            'score': specificity_score,\n",
        "            'technical_matches': technical_matches\n",
        "        }\n",
        "\n",
        "        total_score += specificity_score\n",
        "\n",
        "        # Determine primary reason for confidence level\n",
        "        max_factor = max(confidence_factors.keys(), key=lambda k: confidence_factors[k]['score'])\n",
        "        primary_reason = f\"Based on {max_factor.replace('_', ' ')}\"\n",
        "\n",
        "        if has_insufficient_info:\n",
        "            primary_reason = \"Answer indicates missing information\"\n",
        "        elif retrieval_score < 10:\n",
        "            primary_reason = \"Low relevance of retrieved sources\"\n",
        "        elif completeness_score > 20:\n",
        "            primary_reason = \"Comprehensive answer with good context\"\n",
        "\n",
        "        confidence_factors['primary_reason'] = primary_reason\n",
        "        confidence_factors['breakdown'] = {\n",
        "            'Retrieval Quality': confidence_factors['retrieval_quality']['score'],\n",
        "            'Answer Completeness': confidence_factors['answer_completeness']['score'],\n",
        "            'Context Relevance': confidence_factors['context_relevance']['score'],\n",
        "            'Source Diversity': confidence_factors['source_diversity']['score'],\n",
        "            'Technical Specificity': confidence_factors['technical_specificity']['score']\n",
        "        }\n",
        "\n",
        "        return min(100, total_score), confidence_factors\n",
        "\n",
        "    def _score_to_confidence_label(self, score: int) -> str:\n",
        "        \"\"\"Convert numerical score to confidence label\"\"\"\n",
        "        if score >= 85:\n",
        "            return \"Excellent\"\n",
        "        elif score >= 70:\n",
        "            return \"High\"\n",
        "        elif score >= 50:\n",
        "            return \"Medium\"\n",
        "        elif score >= 30:\n",
        "            return \"Low\"\n",
        "        else:\n",
        "            return \"Very Low\"\n",
        "\n",
        "    def _get_confidence_color(self, score: int) -> str:\n",
        "        \"\"\"Get emoji indicator for confidence level\"\"\"\n",
        "        if score >= 85:\n",
        "            return \"ðŸŸ¢\"  # Green\n",
        "        elif score >= 70:\n",
        "            return \"ðŸ”µ\"  # Blue\n",
        "        elif score >= 50:\n",
        "            return \"ðŸŸ¡\"  # Yellow\n",
        "        elif score >= 30:\n",
        "            return \"ðŸŸ \"  # Orange\n",
        "        else:\n",
        "            return \"ðŸ”´\"  # Red\n",
        "\n",
        "    def _show_detailed_confidence_analysis(self, score: int, breakdown: Dict) -> None:\n",
        "        \"\"\"Show detailed confidence score breakdown\"\"\"\n",
        "\n",
        "        print(f\"\\nðŸ“Š CONFIDENCE ANALYSIS\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"ðŸŽ¯ Overall Confidence Score: {score}/100 ({self._score_to_confidence_label(score)})\")\n",
        "        print(f\"ðŸ“ˆ Score Breakdown:\")\n",
        "\n",
        "        for factor, factor_score in breakdown['breakdown'].items():\n",
        "            percentage = (factor_score / 25) * 100 if factor != 'Technical Specificity' else (factor_score / 10) * 100\n",
        "            bar = \"â–ˆ\" * int(percentage / 10) + \"â–‘\" * (10 - int(percentage / 10))\n",
        "            print(f\"   {factor:20}: {factor_score:2d} [{bar}]\")\n",
        "\n",
        "        print(f\"ðŸ’¡ Primary Factor: {breakdown['primary_reason']}\")\n",
        "\n",
        "    def _show_retrieval_details(self, chunks: List[Dict]) -> None:\n",
        "        \"\"\"Show retrieval analysis details\"\"\"\n",
        "\n",
        "        scores = [chunk['similarity_score'] for chunk in chunks]\n",
        "\n",
        "        print(\"ðŸ“Š Step 2: Analyzing retrieval results...\")\n",
        "        print(f\"   ðŸŽ¯ Best score: {max(scores):.3f}\")\n",
        "        print(f\"   ðŸ“Š Average: {sum(scores)/len(scores):.3f}\")\n",
        "        print(f\"   ðŸ“‰ Worst score: {min(scores):.3f}\")\n",
        "\n",
        "        print(f\"\\n   ðŸ“‹ Top 3 results:\")\n",
        "        for i, chunk in enumerate(chunks[:3], 1):\n",
        "            score = chunk['similarity_score']\n",
        "            confidence = \"High\" if score >= 0.7 else \"Medium\" if score >= 0.4 else \"Low\"\n",
        "            source = chunk['metadata'].get('source_file', 'Unknown')\n",
        "            section = chunk['metadata'].get('section_number', 'N/A')\n",
        "\n",
        "            print(f\"   {i}. {source} (Section {section}) - Score: {score:.3f} ({confidence})\")\n",
        "            print(f\"      Preview: {chunk['content'][:80]}...\")\n",
        "\n",
        "    def _assemble_context(self, chunks: List[Dict], show_details: bool = False) -> tuple[str, List[Dict]]:\n",
        "        \"\"\"Assemble context from retrieved chunks\"\"\"\n",
        "\n",
        "        # Filter chunks by minimum confidence threshold\n",
        "        filtered_chunks = [\n",
        "            chunk for chunk in chunks\n",
        "            if chunk['similarity_score'] >= 0.15  # Keep threshold low to retain context\n",
        "        ]\n",
        "\n",
        "        if not filtered_chunks:\n",
        "            filtered_chunks = chunks[:1]  # Take at least one\n",
        "\n",
        "        if show_details:\n",
        "            print(\"ðŸ”§ Step 3: Assembling context...\")\n",
        "            print(f\"   âœ… Using {len(filtered_chunks)} chunks for context\")\n",
        "\n",
        "        # Build context\n",
        "        context_parts = []\n",
        "        for i, chunk in enumerate(filtered_chunks, 1):\n",
        "            source = chunk['metadata'].get('source_file', 'Migration Documentation')\n",
        "            section = chunk['metadata'].get('section_number', 'N/A')\n",
        "            section_title = chunk['metadata'].get('section_title', '')\n",
        "\n",
        "            context_part = f\"[Source {i}: {source}, Section {section}\"\n",
        "            if section_title:\n",
        "                context_part += f\", Title: {section_title}\"\n",
        "            context_part += f\"]\\n{chunk['content']}\\n\"\n",
        "\n",
        "            context_parts.append(context_part)\n",
        "\n",
        "        context = \"\\n---\\n\".join(context_parts)\n",
        "\n",
        "        if show_details:\n",
        "            print(f\"   ðŸ“ Total context length: {len(context)} characters\")\n",
        "\n",
        "        return context, filtered_chunks\n",
        "\n",
        "    def _show_final_details(self, answer: str, chunks: List[Dict], confidence_score: int) -> None:\n",
        "        \"\"\"Show detailed final results\"\"\"\n",
        "\n",
        "        print(\"\\nðŸŽ¯ FINAL RESULT\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"ðŸ“ Answer: {answer}\")\n",
        "        print(f\"ðŸŽ¯ Confidence: {confidence_score}/100 ({self._score_to_confidence_label(confidence_score)})\")\n",
        "        print(f\"ðŸ“š Sources Used: {len(chunks)}\")\n",
        "\n",
        "        print(f\"\\nðŸ“Š Sources:\")\n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            score = chunk['similarity_score']\n",
        "            source = chunk['metadata'].get('source_file', 'Unknown')\n",
        "            section = chunk['metadata'].get('section_number', 'N/A')\n",
        "            print(f\"   {i}. {source} (Section {section}) - Score: {score:.3f}\")\n",
        "\n",
        "    def _show_clean_result(self, query: str, answer: str, confidence_score: int, chunks: List[Dict], reason: str) -> None:\n",
        "        \"\"\"Show clean result for normal usage with enhanced confidence display\"\"\"\n",
        "\n",
        "        confidence_icon = self._get_confidence_color(confidence_score)\n",
        "        confidence_label = self._score_to_confidence_label(confidence_score)\n",
        "\n",
        "        print(f\"\\nâ“ Question: {query}\")\n",
        "        print(\"â”€\" * 60)\n",
        "        print(f\"ðŸ“– Answer:\\n{answer}\")\n",
        "\n",
        "        # Enhanced confidence display\n",
        "        print(f\"\\n{confidence_icon} Confidence Score: {confidence_score}/100 ({confidence_label})\")\n",
        "        print(f\"ðŸ’¡ Reason: {reason}\")\n",
        "\n",
        "        # Visual confidence bar\n",
        "        filled_bars = int(confidence_score / 10)\n",
        "        empty_bars = 10 - filled_bars\n",
        "        confidence_bar = \"â–ˆ\" * filled_bars + \"â–‘\" * empty_bars\n",
        "        print(f\"ðŸ“Š Score: [{confidence_bar}] {confidence_score}%\")\n",
        "\n",
        "        if chunks:\n",
        "            print(f\"\\nðŸ“š Sources ({len(chunks)}):\")\n",
        "            for i, chunk in enumerate(chunks[:3], 1):  # Show top 3 sources\n",
        "                source = chunk['metadata'].get('source_file', 'Unknown')\n",
        "                section = chunk['metadata'].get('section_number', 'N/A')\n",
        "                score = chunk['similarity_score']\n",
        "                print(f\"   {i}. {source} (Section {section}) - Relevance: {score:.3f}\")\n",
        "\n",
        "        print(\"â”€\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# DEMO FUNCTION\n",
        "# ==========================================\n",
        "\n",
        "def run_demo_example(rag_system):\n",
        "    \"\"\"Run demo example showing confidence scoring\"\"\"\n",
        "\n",
        "    print(\"\\nðŸŽ¬ DEMO EXAMPLE - Migration Question with Confidence Scoring\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    demo_queries = [\n",
        "        \"How do you convert COBOL COMP-3 fields to Java BigDecimal?\",  # Should get high confidence\n",
        "        \"What are the daily transaction limits for different account types?\",  # Should get medium confidence\n",
        "        \"How to deploy Spring Boot apps on Kubernetes?\"  # Should get low confidence (not in migration docs)\n",
        "    ]\n",
        "\n",
        "    for i, query in enumerate(demo_queries, 1):\n",
        "        print(f\"\\n{'ðŸŽ¯ TEST ' + str(i):-^50}\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(\"â–¶ï¸\" * 10 + \" PROCESSING \" + \"â–¶ï¸\" * 10)\n",
        "\n",
        "        answer = rag_system.ask(query, show_details=(i == 1))  # Show details for first query only\n",
        "\n",
        "        if i < len(demo_queries):\n",
        "            input(f\"\\nðŸ‘† Press Enter for next test...\")\n",
        "\n",
        "    print(\"\\nðŸ Demo complete! Notice how confidence scores vary based on:\")\n",
        "    print(\"   âœ… Quality of retrieved sources\")\n",
        "    print(\"   âœ… Completeness of the answer\")\n",
        "    print(\"   âœ… Technical specificity\")\n",
        "    print(\"   âœ… Context relevance\")\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "print(\"ðŸš€ Initializing Enhanced RAG System...\")\n",
        "\n",
        "try:\n",
        "    # Initialize the RAG system\n",
        "    migration_rag = MainframeMigrationRAG(\n",
        "        retrieval_system=retrieval_system,\n",
        "        embedding_model=embedding_model\n",
        "    )\n",
        "\n",
        "    print(\"âœ… Enhanced RAG System Ready with Confidence Scoring!\")\n",
        "    print(\"\\nðŸ’¡ Usage Examples:\")\n",
        "    print(\"   ðŸ“ migration_rag.ask('How to handle overdraft calculations?')\")\n",
        "    print(\"   ðŸ” migration_rag.ask('Your question', show_details=True)\")\n",
        "    print(\"   ðŸŽ¬ run_demo_example(migration_rag)\")\n",
        "\n",
        "    # Optional: Run demo automatically\n",
        "    print(f\"\\nðŸŽ¬ Running confidence scoring demo...\")\n",
        "    demo_result = run_demo_example(migration_rag)\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"âŒ Setup incomplete: {e}\")\n",
        "    print(\"ðŸ“‹ Required from previous phases:\")\n",
        "    print(\"  â€¢ retrieval_system\")\n",
        "    print(\"  â€¢ embedding_model\")\n",
        "    migration_rag = None\n",
        "\n",
        "# Convenience function\n",
        "def ask(query: str, details: bool = False):\n",
        "    \"\"\"Convenience function for quick testing\"\"\"\n",
        "    if migration_rag:\n",
        "        return migration_rag.ask(query, show_details=details)\n",
        "    else:\n",
        "        print(\"âŒ RAG system not initialized\")\n",
        "\n",
        "print(f\"\\nðŸŽ¤ READY FOR USE!\")\n",
        "print(f\"ðŸ’¡ Quick test: ask('How to convert COBOL decimals to Java?')\")\n",
        "print(f\"ðŸ” With details: ask('Your question', details=True)\")"
      ],
      "metadata": {
        "id": "hcuT3FaAEZJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agentic RAG with Tavily AWS Search"
      ],
      "metadata": {
        "id": "g2hHOs55Z_qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "from enum import Enum\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"ðŸ¤– Complete Agentic RAG with Tavily AWS Search\")\n",
        "print(\"Mainframe Migration - Internal Docs + AWS Documentation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ==========================================\n",
        "# TAVILY AWS-FOCUSED SEARCH INTEGRATION\n",
        "# ==========================================\n",
        "\n",
        "class TavilySearcher:\n",
        "    \"\"\"Tavily AWS-focused search integration\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://api.tavily.com/search\"\n",
        "\n",
        "        # AWS-specific domains only\n",
        "        self.aws_domains = [\n",
        "            \"docs.aws.amazon.com\",\n",
        "            \"aws.amazon.com\",\n",
        "            \"github.com/aws-samples\",\n",
        "            \"github.com/awslabs\",\n",
        "            \"repost.aws\",\n",
        "            \"aws.amazon.com/blogs\"\n",
        "        ]\n",
        "\n",
        "    def search(self, query: str, max_results: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"Search AWS documentation using Tavily API\"\"\"\n",
        "\n",
        "        try:\n",
        "            payload = {\n",
        "                \"api_key\": self.api_key,\n",
        "                \"query\": f\"{query} AWS mainframe migration COBOL Java\",  # Add context\n",
        "                \"search_depth\": \"advanced\",\n",
        "                \"include_answer\": True,\n",
        "                \"include_raw_content\": True,\n",
        "                \"max_results\": max_results,\n",
        "                \"include_images\": False,\n",
        "                \"include_domains\": self.aws_domains  # AWS-only search\n",
        "            }\n",
        "\n",
        "            response = requests.post(self.base_url, json=payload, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "\n",
        "            # Filter results to ensure they're from AWS domains\n",
        "            if 'results' in result:\n",
        "                filtered_results = []\n",
        "                for r in result['results']:\n",
        "                    url = r.get('url', '')\n",
        "                    if any(domain in url for domain in self.aws_domains):\n",
        "                        filtered_results.append(r)\n",
        "                result['results'] = filtered_results\n",
        "\n",
        "            return result\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"âŒ Tavily AWS search error: {e}\")\n",
        "            return {\"results\": [], \"answer\": None, \"error\": str(e)}\n",
        "\n",
        "# ==========================================\n",
        "# MIGRATION QUERY ROUTING ENGINE\n",
        "# ==========================================\n",
        "\n",
        "class MigrationQueryRouter:\n",
        "    \"\"\"Intelligent routing between internal docs and AWS documentation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # AWS documentation trigger patterns\n",
        "        self.aws_patterns = {\n",
        "            'aws_services': [\n",
        "                r'\\b(aws\\s+transform|amazon\\s+q|aws\\s+mainframe\\s+modernization)\\b',\n",
        "                r'\\b(bedrock|kendra|opensearch|rds|ec2|lambda)\\b',\n",
        "                r'\\b(aws\\s+batch|step\\s+functions|api\\s+gateway)\\b',\n",
        "                r'\\baws\\s+.*(service|pricing|feature|limit|quota)\\b',\n",
        "                r'\\b(cloudformation|cloudwatch|iam|s3|vpc)\\b'\n",
        "            ],\n",
        "            'current_aws_info': [\n",
        "                r'\\b(latest|newest|current|recent).*\\baws\\b',\n",
        "                r'\\baws\\s+.*(2024|2025|version|update|release)\\b',\n",
        "                r'\\b(new|recent)\\s+.*(aws|amazon)\\s+.*(feature|service)\\b',\n",
        "                r'\\bwhat.*new.*aws\\b'\n",
        "            ],\n",
        "            'aws_best_practices': [\n",
        "                r'\\baws\\s+.*(best\\s+practice|recommendation|guidance)\\b',\n",
        "                r'\\b(official|aws)\\s+.*(guide|documentation|recommendation)\\b',\n",
        "                r'\\bhow\\s+does\\s+aws\\s+recommend\\b',\n",
        "                r'\\baws\\s+.*(architecture|pattern|framework)\\b'\n",
        "            ],\n",
        "            'aws_specific_migration': [\n",
        "                r'\\baws\\s+transform\\s+.*(how|steps|process|guide)\\b',\n",
        "                r'\\bamazon\\s+q\\s+developer\\s+.*(migration|conversion)\\b',\n",
        "                r'\\baws\\s+mainframe\\s+modernization\\s+service\\b',\n",
        "                r'\\b(aws\\s+transform|amazon\\s+q)\\s+.*(capabilities|features)\\b'\n",
        "            ],\n",
        "            'aws_tools_comparison': [\n",
        "                r'\\bcompare\\s+.*aws.*\\b(tools|services)\\b',\n",
        "                r'\\baws\\s+transform\\s+vs\\s+.*(micro\\s+focus|tsri|ibm)\\b',\n",
        "                r'\\bwhich\\s+aws\\s+service\\s+for\\b'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Internal documentation trigger patterns\n",
        "        self.internal_patterns = {\n",
        "            'company_experience': [\n",
        "                r'\\b(our|we|company|organization)\\s+.*(experience|approach|decision)\\b',\n",
        "                r'\\b(lessons\\s+learned|what\\s+worked|what\\s+failed)\\b',\n",
        "                r'\\bhow\\s+(did\\s+we|do\\s+we)\\s+.*(migrate|convert|handle)\\b',\n",
        "                r'\\b(our\\s+migration|our\\s+project|our\\s+experience)\\b'\n",
        "            ],\n",
        "            'specific_systems': [\n",
        "                r'\\b(accval01|cams|customer\\s+account\\s+management)\\b',\n",
        "                r'\\b(cobol\\s+program|copybook|jcl\\s+job)\\b.*\\b(specification|documentation)\\b',\n",
        "                r'\\bmigration\\s+playbook\\b',\n",
        "                r'\\b(architecture\\s+decision|adr)\\b'\n",
        "            ],\n",
        "            'troubleshooting_internal': [\n",
        "                r'\\bhow\\s+to\\s+(fix|resolve|troubleshoot)\\s+.*(error|issue|problem)\\b',\n",
        "                r'\\b(decimal\\s+precision|comp-3|overdraft\\s+calculation)\\s+.*(error|issue)\\b',\n",
        "                r'\\b(connection\\s+timeout|memory\\s+leak|performance\\s+degradation)\\b',\n",
        "                r'\\berror.*\\b(during|after)\\s+migration\\b'\n",
        "            ],\n",
        "            'internal_standards': [\n",
        "                r'\\b(coding\\s+standard|development\\s+guideline|naming\\s+convention)\\b',\n",
        "                r'\\bwhat\\s+.*(pattern|framework|library)\\s+should\\s+(we|i)\\s+use\\b',\n",
        "                r'\\b(java\\s+development\\s+standard|spring\\s+boot\\s+pattern)\\b',\n",
        "                r'\\btesting\\s+strategy\\b.*migration\\b'\n",
        "            ],\n",
        "            'business_rules': [\n",
        "                r'\\b(business\\s+rule|validation\\s+rule|account\\s+validation)\\b',\n",
        "                r'\\b(daily\\s+limit|transaction\\s+limit|overdraft\\s+limit)\\b',\n",
        "                r'\\bhow\\s+does.*\\b(accval01|validation\\s+program)\\b.*work\\b'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Response quality indicators for fallback decisions\n",
        "        self.poor_response_indicators = [\n",
        "            \"does not specifically mention\",\n",
        "            \"refer to external documentation\",\n",
        "            \"consult the official documentation\",\n",
        "            \"i don't have information\",\n",
        "            \"not available in the provided context\",\n",
        "            \"cannot be found in the documentation\",\n",
        "            \"not covered in detail\",\n",
        "            \"for more information\",\n",
        "            \"additional resources needed\"\n",
        "        ]\n",
        "\n",
        "    def route_query(self, query: str) -> tuple[str, str]:\n",
        "        \"\"\"\n",
        "        Decide routing: 'internal', 'aws', or 'hybrid'\n",
        "        Returns: (route_decision, reason)\n",
        "        \"\"\"\n",
        "\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Check for AWS patterns first (more specific)\n",
        "        for category, patterns in self.aws_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                if re.search(pattern, query_lower):\n",
        "                    return 'aws', f\"AWS pattern match: {category}\"\n",
        "\n",
        "        # Check for internal patterns\n",
        "        for category, patterns in self.internal_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                if re.search(pattern, query_lower):\n",
        "                    return 'internal', f\"Internal pattern match: {category}\"\n",
        "\n",
        "        # Default routing based on query content\n",
        "        aws_terms = ['aws', 'amazon', 'cloud', 'service', 'official', 'documentation']\n",
        "        internal_terms = ['our', 'we', 'company', 'internal', 'playbook', 'experience', 'lesson']\n",
        "\n",
        "        aws_score = sum(1 for term in aws_terms if term in query_lower)\n",
        "        internal_score = sum(1 for term in internal_terms if term in query_lower)\n",
        "\n",
        "        if aws_score > internal_score:\n",
        "            return 'aws', f\"AWS-related terms detected (score: {aws_score})\"\n",
        "        elif internal_score > aws_score:\n",
        "            return 'internal', f\"Internal-related terms detected (score: {internal_score})\"\n",
        "        else:\n",
        "            return 'hybrid', \"Ambiguous query - trying both sources\"\n",
        "\n",
        "    def should_try_aws_fallback(self, internal_response: str) -> tuple[bool, str]:\n",
        "        \"\"\"Analyze if internal response needs AWS fallback\"\"\"\n",
        "\n",
        "        if not internal_response or len(internal_response.strip()) < 30:\n",
        "            return True, \"Internal response too short\"\n",
        "\n",
        "        response_lower = internal_response.lower()\n",
        "\n",
        "        # Check for poor response indicators\n",
        "        for indicator in self.poor_response_indicators:\n",
        "            if indicator in response_lower:\n",
        "                return True, f\"Internal response insufficient: {indicator}\"\n",
        "\n",
        "        return False, \"Internal response appears sufficient\"\n",
        "\n",
        "# ==========================================\n",
        "# COMPLETE AGENTIC RAG SYSTEM\n",
        "# ==========================================\n",
        "\n",
        "class MigrationAgenticRAG:\n",
        "    \"\"\"Complete Agentic RAG for mainframe migration with intelligent routing\"\"\"\n",
        "\n",
        "    def __init__(self, internal_rag_system, tavily_api_key: str, show_internals: bool = True):\n",
        "        self.internal_rag = internal_rag_system\n",
        "        self.aws_searcher = TavilySearcher(tavily_api_key)\n",
        "        self.router = MigrationQueryRouter()\n",
        "        self.show_internals = show_internals\n",
        "\n",
        "        # Disable internals for internal RAG to reduce noise\n",
        "        if hasattr(self.internal_rag, 'show_internals'):\n",
        "            self.internal_rag.show_internals = False\n",
        "\n",
        "    def answer_question(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main agentic pipeline with intelligent routing\"\"\"\n",
        "\n",
        "        if self.show_internals:\n",
        "            print(f\"\\nðŸ¤– MIGRATION AGENTIC RAG: '{query}'\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "        # Stage 1: Routing decision\n",
        "        route_decision, route_reason = self.router.route_query(query)\n",
        "\n",
        "        if self.show_internals:\n",
        "            print(f\"ðŸ” Stage 1: Routing Analysis\")\n",
        "            print(f\"   ðŸŽ¯ Decision: {route_decision.upper()}\")\n",
        "            print(f\"   ðŸ’¡ Reason: {route_reason}\")\n",
        "\n",
        "        # Execute based on routing decision\n",
        "        if route_decision == 'internal':\n",
        "            return self._internal_only_response(query, route_reason)\n",
        "        elif route_decision == 'aws':\n",
        "            return self._aws_only_response(query, route_reason)\n",
        "        else:  # hybrid\n",
        "            return self._hybrid_response(query, route_reason)\n",
        "\n",
        "    def _internal_only_response(self, query: str, reason: str) -> Dict[str, Any]:\n",
        "        \"\"\"Use only internal documentation\"\"\"\n",
        "\n",
        "        if self.show_internals:\n",
        "            print(\"\\nðŸ“š Stage 2: Using INTERNAL documentation only\")\n",
        "\n",
        "        try:\n",
        "            internal_result = self.internal_rag.ask(query, show_details=False)\n",
        "\n",
        "            # Check if we should fallback to AWS anyway\n",
        "            should_fallback, fallback_reason = self.router.should_try_aws_fallback(internal_result)\n",
        "\n",
        "            if should_fallback:\n",
        "                if self.show_internals:\n",
        "                    print(f\"   ðŸ”„ Internal insufficient: {fallback_reason}\")\n",
        "                    print(\"   ðŸŒ Falling back to AWS search...\")\n",
        "                return self._hybrid_response(query, f\"Internal fallback: {fallback_reason}\")\n",
        "\n",
        "            if self.show_internals:\n",
        "                print(f\"   âœ… Internal response sufficient\")\n",
        "\n",
        "            return {\n",
        "                'query': query,\n",
        "                'answer': internal_result,\n",
        "                'routing_decision': 'internal_only',\n",
        "                'routing_reason': reason,\n",
        "                'source_type': 'Internal Migration Documentation',\n",
        "                'confidence_source': 'Internal Knowledge Base',\n",
        "                'aws_searched': False,\n",
        "                'internal_searched': True\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.show_internals:\n",
        "                print(f\"   âŒ Internal search failed: {e}\")\n",
        "            return self._aws_fallback_response(query, f\"Internal search failed: {e}\")\n",
        "\n",
        "    def _aws_only_response(self, query: str, reason: str) -> Dict[str, Any]:\n",
        "        \"\"\"Use only AWS documentation via Tavily\"\"\"\n",
        "\n",
        "        if self.show_internals:\n",
        "            print(\"\\nðŸŒ Stage 2: Using AWS documentation only\")\n",
        "\n",
        "        aws_result = self.aws_searcher.search(query, max_results=5)\n",
        "\n",
        "        if self.show_internals:\n",
        "            result_count = len(aws_result.get('results', []))\n",
        "            print(f\"   ðŸ“Š Found {result_count} AWS results\")\n",
        "\n",
        "            if aws_result.get('answer'):\n",
        "                print(f\"   ðŸ“ Direct AWS answer available\")\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'answer': self._format_aws_answer(aws_result),\n",
        "            'routing_decision': 'aws_only',\n",
        "            'routing_reason': reason,\n",
        "            'source_type': 'AWS Official Documentation',\n",
        "            'confidence_source': 'AWS Authoritative Sources',\n",
        "            'aws_results_count': len(aws_result.get('results', [])),\n",
        "            'aws_sources': self._format_aws_sources(aws_result),\n",
        "            'aws_searched': True,\n",
        "            'internal_searched': False\n",
        "        }\n",
        "\n",
        "    def _hybrid_response(self, query: str, reason: str) -> Dict[str, Any]:\n",
        "        \"\"\"Combine internal and AWS sources\"\"\"\n",
        "\n",
        "        if self.show_internals:\n",
        "            print(\"\\nðŸ”„ Stage 2: Using HYBRID approach (Internal + AWS)\")\n",
        "\n",
        "        # Try internal first\n",
        "        internal_result = None\n",
        "        internal_success = False\n",
        "\n",
        "        if self.show_internals:\n",
        "            print(\"   ðŸ“š Searching internal documentation...\")\n",
        "\n",
        "        try:\n",
        "            internal_result = self.internal_rag.ask(query, show_details=False)\n",
        "            internal_success = True\n",
        "            if self.show_internals:\n",
        "                print(f\"   âœ… Internal search completed\")\n",
        "        except Exception as e:\n",
        "            internal_result = f\"Internal documentation search failed: {e}\"\n",
        "            if self.show_internals:\n",
        "                print(f\"   âŒ Internal search failed: {e}\")\n",
        "\n",
        "        # Get AWS information\n",
        "        if self.show_internals:\n",
        "            print(\"   ðŸŒ Searching AWS documentation...\")\n",
        "\n",
        "        aws_result = self.aws_searcher.search(query, max_results=3)\n",
        "        aws_answer = self._format_aws_answer(aws_result)\n",
        "\n",
        "        if self.show_internals:\n",
        "            aws_count = len(aws_result.get('results', []))\n",
        "            print(f\"   ðŸ“Š AWS search found {aws_count} results\")\n",
        "\n",
        "        # Combine responses intelligently\n",
        "        if internal_success and len(internal_result.strip()) > 50:\n",
        "            combined_answer = f\"\"\"**From Your Internal Migration Experience:**\n",
        "{internal_result}\n",
        "\n",
        "**From AWS Official Documentation:**\n",
        "{aws_answer}\n",
        "\n",
        "---\n",
        "*This response combines your organization's migration experience with current AWS guidance.*\"\"\"\n",
        "        else:\n",
        "            combined_answer = f\"\"\"**From AWS Official Documentation:**\n",
        "{aws_answer}\n",
        "\n",
        "**Note:** Internal documentation search was {'unsuccessful' if not internal_success else 'limited'}. This response relies primarily on AWS official sources.\n",
        "\n",
        "---\n",
        "*For complete guidance, ensure your internal migration documentation is accessible.*\"\"\"\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'answer': combined_answer,\n",
        "            'routing_decision': 'hybrid',\n",
        "            'routing_reason': reason,\n",
        "            'source_type': 'Internal + AWS Documentation',\n",
        "            'confidence_source': 'Combined Sources',\n",
        "            'aws_results_count': len(aws_result.get('results', [])),\n",
        "            'aws_sources': self._format_aws_sources(aws_result),\n",
        "            'internal_success': internal_success,\n",
        "            'aws_searched': True,\n",
        "            'internal_searched': True\n",
        "        }\n",
        "\n",
        "    def _aws_fallback_response(self, query: str, fallback_reason: str) -> Dict[str, Any]:\n",
        "        \"\"\"Fallback to AWS when internal fails\"\"\"\n",
        "\n",
        "        if self.show_internals:\n",
        "            print(f\"\\nðŸŒ AWS Fallback: {fallback_reason}\")\n",
        "\n",
        "        aws_result = self.aws_searcher.search(query, max_results=5)\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'answer': self._format_aws_answer(aws_result),\n",
        "            'routing_decision': 'aws_fallback',\n",
        "            'routing_reason': fallback_reason,\n",
        "            'source_type': 'AWS Documentation (Fallback)',\n",
        "            'confidence_source': 'AWS Fallback',\n",
        "            'aws_results_count': len(aws_result.get('results', [])),\n",
        "            'aws_searched': True,\n",
        "            'internal_searched': False\n",
        "        }\n",
        "\n",
        "    def _format_aws_answer(self, aws_result: Dict) -> str:\n",
        "        \"\"\"Format answer from AWS search results\"\"\"\n",
        "\n",
        "        # Use Tavily's direct answer if available and good quality\n",
        "        if aws_result.get('answer') and len(aws_result['answer'].strip()) > 100:\n",
        "            return aws_result['answer']\n",
        "\n",
        "        # Otherwise combine top results\n",
        "        if aws_result.get('results'):\n",
        "            combined_content = []\n",
        "            for i, result in enumerate(aws_result['results'][:3], 1):\n",
        "                title = result.get('title', f'AWS Source {i}')\n",
        "                content = result.get('content', '').strip()\n",
        "\n",
        "                if content:\n",
        "                    # Clean and truncate content\n",
        "                    content = content[:400] + \"...\" if len(content) > 400 else content\n",
        "                    combined_content.append(f\"**{title}:**\\n{content}\")\n",
        "\n",
        "            if combined_content:\n",
        "                formatted_answer = \"\\n\\n\".join(combined_content)\n",
        "                formatted_answer += \"\\n\\n*Based on current AWS documentation and resources.*\"\n",
        "                return formatted_answer\n",
        "\n",
        "        # Fallback if no good content\n",
        "        if aws_result.get('error'):\n",
        "            return f\"AWS documentation search encountered an error: {aws_result['error']}\"\n",
        "        else:\n",
        "            return \"I found AWS resources but couldn't extract comprehensive information. Please check the source links provided.\"\n",
        "\n",
        "    def _format_aws_sources(self, aws_result: Dict) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Format AWS search sources for display\"\"\"\n",
        "\n",
        "        sources = []\n",
        "        for result in aws_result.get('results', []):\n",
        "            source = {\n",
        "                'title': result.get('title', 'AWS Documentation'),\n",
        "                'url': result.get('url', 'No URL'),\n",
        "                'domain': self._extract_domain(result.get('url', '')),\n",
        "                'type': 'aws_official',\n",
        "                'content_preview': result.get('content', '')[:100] + \"...\" if result.get('content') else 'No preview'\n",
        "            }\n",
        "            sources.append(source)\n",
        "\n",
        "        return sources\n",
        "\n",
        "    def _extract_domain(self, url: str) -> str:\n",
        "        \"\"\"Extract domain from URL\"\"\"\n",
        "        try:\n",
        "            from urllib.parse import urlparse\n",
        "            return urlparse(url).netloc\n",
        "        except:\n",
        "            return 'aws.amazon.com'\n",
        "\n",
        "    def show_routing_summary(self, result: Dict[str, Any]) -> None:\n",
        "        \"\"\"Display routing decision summary\"\"\"\n",
        "\n",
        "        print(f\"\\nðŸ“Š ROUTING SUMMARY\")\n",
        "        print(\"=\" * 40)\n",
        "        print(f\"ðŸŽ¯ Decision: {result['routing_decision']}\")\n",
        "        print(f\"ðŸ’¡ Reason: {result['routing_reason']}\")\n",
        "        print(f\"ðŸ“š Source Type: {result['source_type']}\")\n",
        "        print(f\"ðŸ” Internal Searched: {'Yes' if result.get('internal_searched') else 'No'}\")\n",
        "        print(f\"ðŸŒ AWS Searched: {'Yes' if result.get('aws_searched') else 'No'}\")\n",
        "\n",
        "        if result.get('aws_results_count'):\n",
        "            print(f\"ðŸ“Š AWS Results: {result['aws_results_count']}\")\n",
        "\n",
        "# ==========================================\n",
        "# SETUP AND INITIALIZATION\n",
        "# ==========================================\n",
        "\n",
        "def setup_migration_agentic_rag():\n",
        "    \"\"\"Setup the complete migration agentic RAG system\"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Setting up Migration Agentic RAG System...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Get Tavily API key\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        tavily_api_key = userdata.get('TAVILY_API_KEY')\n",
        "        print(\"âœ… Tavily API key loaded from Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading Tavily API key from secrets: {e}\")\n",
        "        print(\"ðŸ”‘ Please add TAVILY_API_KEY to Colab secrets\")\n",
        "        print(\"ðŸ“ Get your free key from: https://tavily.com\")\n",
        "\n",
        "        # Allow manual input as fallback\n",
        "        tavily_api_key = input(\"Enter Tavily API key manually (or press Enter to skip): \").strip()\n",
        "        if not tavily_api_key:\n",
        "            print(\"âŒ No Tavily API key provided - AWS search will not work\")\n",
        "            return None\n",
        "\n",
        "    # Test Tavily connection with AWS search\n",
        "    print(\"ðŸ”„ Testing Tavily AWS search connection...\")\n",
        "    test_searcher = TavilySearcher(tavily_api_key)\n",
        "    test_result = test_searcher.search(\"AWS Transform mainframe migration\", max_results=1)\n",
        "\n",
        "    if test_result.get('error'):\n",
        "        print(f\"âŒ Tavily test failed: {test_result['error']}\")\n",
        "        return None\n",
        "    else:\n",
        "        result_count = len(test_result.get('results', []))\n",
        "        print(f\"âœ… Tavily AWS search successful! Found {result_count} results\")\n",
        "\n",
        "    # Initialize agentic RAG system\n",
        "    try:\n",
        "        agentic_rag = MigrationAgenticRAG(\n",
        "            internal_rag_system=migration_rag,  # From previous phase\n",
        "            tavily_api_key=tavily_api_key,\n",
        "            show_internals=True\n",
        "        )\n",
        "\n",
        "        print(\"âœ… Migration Agentic RAG system initialized!\")\n",
        "        print(\"ðŸŽ¯ Ready for intelligent internal/AWS routing!\")\n",
        "        return agentic_rag\n",
        "\n",
        "    except NameError as e:\n",
        "        print(f\"âŒ Missing component: {e}\")\n",
        "        print(\"ðŸ“‹ Make sure previous phases are complete:\")\n",
        "        print(\"   â€¢ migration_rag (from Phase 1D enhanced)\")\n",
        "        return None\n",
        "\n",
        "# ==========================================\n",
        "# DEMO AND TESTING\n",
        "# ==========================================\n",
        "\n",
        "def demo_migration_routing():\n",
        "    \"\"\"Demonstrate intelligent routing with migration-specific queries\"\"\"\n",
        "\n",
        "    demo_queries = [\n",
        "        # Demo 1: Should route to INTERNAL docs\n",
        "        \"What were the lessons learned from our CAMS migration?\",\n",
        "\n",
        "        # Demo 2: Should route to AWS docs\n",
        "        \"What are the latest AWS Transform features for COBOL conversion?\",\n",
        "\n",
        "        # Demo 3: Should use HYBRID approach\n",
        "        \"Best practices for COBOL to Java migration\"\n",
        "    ]\n",
        "\n",
        "    print(\"ðŸŽ­ MIGRATION AGENTIC ROUTING DEMONSTRATION (3 Examples)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        agentic_system = setup_migration_agentic_rag()\n",
        "        if not agentic_system:\n",
        "            return None\n",
        "\n",
        "        for i, query in enumerate(demo_queries, 1):\n",
        "            print(f\"\\n{'ðŸŽ¬ DEMO ' + str(i) + ': ' + query:-^70}\")\n",
        "\n",
        "            # Show expected routing\n",
        "            route_descriptions = [\n",
        "                \"Expected: INTERNAL (company-specific experience)\",\n",
        "                \"Expected: AWS (current AWS service features)\",\n",
        "                \"Expected: HYBRID (combines internal + AWS guidance)\"\n",
        "            ]\n",
        "            print(f\"ðŸ’¡ {route_descriptions[i-1]}\")\n",
        "\n",
        "            result = agentic_system.answer_question(query)\n",
        "\n",
        "            # Show routing summary\n",
        "            agentic_system.show_routing_summary(result)\n",
        "\n",
        "            # Show answer preview\n",
        "            answer_preview = result['answer'][:200] + \"...\" if len(result['answer']) > 200 else result['answer']\n",
        "            print(f\"\\nðŸ“– Answer Preview:\\n{answer_preview}\")\n",
        "\n",
        "            if i < len(demo_queries):\n",
        "                input(f\"\\nðŸ‘† Press Enter to continue to demo {i+1}...\")\n",
        "\n",
        "        print(\"\\nðŸŽ‰ Migration agentic routing demo complete!\")\n",
        "        print(\"ðŸŽ¯ The system intelligently routes between:\")\n",
        "        print(\"   ðŸ“š Internal migration experience & documentation\")\n",
        "        print(\"   ðŸŒ AWS official documentation & services\")\n",
        "        print(\"   ðŸ”„ Hybrid approach combining both sources\")\n",
        "\n",
        "        return agentic_system\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Demo failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "print(\"ðŸ¤– Initializing Complete Migration Agentic RAG System...\")\n",
        "\n",
        "# Run demo automatically\n",
        "migration_agentic_system = demo_migration_routing()\n",
        "\n",
        "if migration_agentic_system:\n",
        "    print(f\"\\nðŸŽ¯ MIGRATION AGENTIC RAG READY!\")\n",
        "    print(f\"ðŸ’¡ Usage: migration_agentic_system.answer_question('your query')\")\n",
        "    print(f\"ðŸŽ“ Test different query types to see intelligent routing!\")\n",
        "\n",
        "    # Store for easy access\n",
        "    agentic_rag = migration_agentic_system\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Migration Agentic RAG setup failed. Check errors above.\")\n",
        "    agentic_rag = None\n",
        "\n",
        "# Convenience functions\n",
        "def ask_migration(query: str):\n",
        "    \"\"\"Quick test function for migration agentic RAG\"\"\"\n",
        "    if agentic_rag:\n",
        "        result = agentic_rag.answer_question(query)\n",
        "        return result\n",
        "    else:\n",
        "        print(\"âŒ Migration Agentic RAG system not initialized\")\n",
        "\n",
        "def test_routing(query: str):\n",
        "    \"\"\"Test routing decision without full processing\"\"\"\n",
        "    if agentic_rag:\n",
        "        route, reason = agentic_rag.router.route_query(query)\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(f\"Route: {route.upper()}\")\n",
        "        print(f\"Reason: {reason}\")\n",
        "        return route, reason\n",
        "    else:\n",
        "        print(\"âŒ System not initialized\")\n",
        "\n",
        "print(f\"\\nðŸŽ¤ READY FOR INTELLIGENT MIGRATION ASSISTANCE!\")\n",
        "print(f\"ðŸ’¡ ask_migration('How does AWS Transform handle COBOL conversion?')\")\n",
        "print(f\"ðŸ’¡ ask_migration('What were our lessons learned from migration?')\")\n",
        "print(f\"ðŸ” test_routing('your query') - See routing decision only\")"
      ],
      "metadata": {
        "id": "eRkEhML8L96U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Agentic RAG"
      ],
      "metadata": {
        "id": "Au39ehCEaRfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_migration('How does our ACCVAL01 program handle overdraft calculations?')"
      ],
      "metadata": {
        "id": "GIF_sxg4sDa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_migration(\"What are the latest Amazon Q Developer features?\")"
      ],
      "metadata": {
        "id": "cD1C2OB4XbXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_migration(\"Best practices for COBOL to Java migration?\")"
      ],
      "metadata": {
        "id": "OyPPaSuURvUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}